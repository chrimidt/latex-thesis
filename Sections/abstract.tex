Long-tailed datasets, characterized by few classes with many samples followed by many classes with few samples, pose significant challenges for image classification tasks. This thesis investigates deep learning techniques tailored to address long-tailed class distributions, focusing on class-sensitive learning and model architecture selection. Four state-of-the-art models (ResNet-50, MobileNetV2, ConvNeXt-Base, and ViT-B/16) are trained on both balanced and long-tailed versions of the CIFAR-100 dataset, where the balanced training serves as a baseline for the performances on the long-tailed training. Furthermore, the study evaluates the loss functions; Softmax Cross-Entropy Loss, Weighted Softmax Cross-Entropy Loss, Focal Loss, Class-Balanced Loss, Balanced Softmax Loss, LDAM Loss, and Equalization Loss, to assess their effectiveness in improving performance on tail classes while maintaining accuracy across head and middle classes. 

The goal is to provide insights into the interplay between loss design and model architecture, offering practical recommendations for handling long-tailed distributions in real-world datasets. Experiments revealed that Balanced Softmax Loss consistently surpassed other loss designs on tail classes while preserving accuracy on head classes when trained with Convolutional Neural Network (CNN) architectures with the best tail-class performance of 60.53\% top-1 accuracy on ResNet-50. Additionally, the Vistion Transformer (ViT) architecture, ViT-B/16, persistently underperformed on both the balanced and long-tailed versions of CIFAR-100, not only lagging far behind the CNN-based architectures but also failing to meet the published benchmark. This highlights the need for further optimization to accomodate for small-scaled datasets, like CIFAR-100, and to unlock the full potential of the ViT models. Finally, the performance of Equalization Loss across various model architectures reveals the importance of careful selection of model and loss design, as it underperformance on all models except the more robust ConvNeXt-Base, emphasizing ConvNeXt's overall ability to maintain performance when paired with different loss designs.

These results contributes to the understanding of long-tailed learning and guides practitioners in selecting effective strategies for addressing class imbalance.

