Long-tailed datasets, characterized by a few classes with abundant samples and many classes with sparse representation, pose significant challenges for image classification tasks. This thesis investigates deep learning techniques tailored to address class imbalance, focusing on class-sensitive learning and model architecture selection. Four state-of-the-art models; ResNet-50, MobileNetV2, ConvNeXt-Base, and ViT-B/16 are trained on both balanced and long-tailed versions of the CIFAR-100 dataset.

The study evaluates various loss functions, including Cross-Entropy Loss, Weighted Cross-Entropy Loss, Focal Loss, Class-Balanced Loss, Balanced Softmax Loss, LDAM Loss, and Equalization Loss, to assess their effectiveness in improving performance on tail classes while maintaining accuracy across head and middle classes. Performance is analyzed using top-1 accuracy, with particular attention to head and tail classes.

The findings provide insights into the interplay between loss design and model architecture, offering practical recommendations for handling long-tailed distributions in real-world datasets. This work contributes to the understanding of long-tailed learning and guides practitioners in selecting effective strategies for addressing class imbalance.

