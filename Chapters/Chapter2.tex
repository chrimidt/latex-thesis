% Chapter 2: Background

% Length: aim for 10-15 pages

This chapter presents the different background topics of the thesis work, which are the long-tailed datasets, model architectures \textit{Convolutional Neural Networks (CNN)} and \textit{Visual Transformers (VT)}, the deep long-tailed learning methods \textit{Class Re-balancing (CR)}, \textit{Information Augmentation (IA)}, 
and \textit{Module Improvement (MI)}. These topics will be explained for the reader.\\

Mention image classification, as it is the primary goal of this thesis. 

\section{Long-Tailed Dataset}
A short introductory paragraph explaining why these topics are relevant and how they tie into the thesis.
Contextualize the sectionsâ€”e.g., "Long-tailed datasets are central to this thesis, as they represent the primary challenge. Model architectures and classic long-tailed methods form the foundation of the approaches explored in this work."\\

Describe the difference between class-imbalanced learning and long-tailed learning.

\section{Model Architechtures}
Briefly describe the role of deep learning models in handling long-tailed datasets.

\subsection{Convolutional Neural Networks}
Add historical context (e.g., their success in computer vision tasks).
Highlight specific CNNs used in this thesis (e.g., ResNet, MobileNet).

\subsection{Visual Transformers}
Explain their advantages over CNNs for certain tasks.
Mention why they are relevant for handling long-tailed datasets.


\section{Classic Long-Tailed Methods}
Introduce the three methods (CR, IA, MI) with a brief explanation of their purpose.\\

Following the paper \textit{Deep Long-Tailed Learning: A Survey} \cite{zhang2023deep}, the existing deep long-tailed learning methods are grouped into three main categories based on their technical approach: class re-balancing, information augmentation, and module improvement. These categories are further divided onto sub-categories: re-sampling, class-sensitive learning, logit adjustment, transfer learning, data augmentation, representation learning, classifier desing, decoupled training, and ensemble learning (TODO: create a figure like figure 2 in the paper). This thesis does not aim to examine all the beforementioned method, but aims to find a deep learning approach to a specific problem. The backgrounds of the methods used in this thesis are described in this section.

\subsection{Class Re-balancing}
The class re-balancing method aims to re-balance the effect of the imbalanced training dataset, and has three main sub-categories: re-sampling, class-sensitive learning, and logit adjustment \cite{zhang2023deep}. 

\subsubsection{Re-sampling}
The traditional way to sample when training deep networks is bases on mini-batch gradient descent with random sampling. This means that each sample has an equal probability of being sampled. When sampling from an imbalanced dataset, samples from head classes naturally occur more often, and thus have higher chance of being sampled than samples from tail classes, making the resulting deep models biased towards head classes. Re-sampling is a method that adresses this problem by adjusting the number of samples per class in each sample batch for model training. 

\subsubsection{Class-sensitive Learning}
Class-sensitive learning incorporates strategies to adjust the loss function, making it more sensitive to the imbalanced nature of the dataset. This approach directly modifies the optimization process to prioritize learning from under-represented tail classes.


\subsubsection{Loss Functions for Class-Sensitive Learning}
The loss function serves as a measure of the model's fitness to the data, quantifying the distance between the actual and predicted values of the target. Typically, the loss is represented as a nonnegative value, where smaller values indicate a better fit, and a perfect fit corresponds to a loss of zero \cite{zhang2023dive}.

Conventional training of deep networks using the softmax cross-entropy loss often overlooks class imbalance. This results in uneven gradients for different classes, leading to suboptimal performance on underrepresented classes. To mitigate this issue, modifications to the loss function are introduced to ensure a more balanced contribution from each class during training. One such technique is re-weighting which adjusts the training loss for different classes by assigning a specific weight to each class \cite{zhang2023deep}. The softmax cross-entropy loss is used as a baseline, and is described below along with the loss functions for re-weighting.

\myindent \textbf{Softmax Cross-Entropy Loss}
The \textit{Softmax-Cross-Entropy loss}, often denoted as \textit{softmax loss}, is a widely used combination for training deep neural networks in classification tasks, including image classification. It is particularly effective for multi-class problems, where the goal is to assign an input image to one of several predefined categories \cite{cs231n} \cite{pytorch_crossentropy}.

The \textit{Softmax} function transforms the raw output scores (logits) of the final layer of a neural network into a probability distribution over \( K \) classes. For an input \( \mathbf{z} = [z_1, z_2, \dots, z_K] \), the Softmax function for class \( i \) is defined as:

\begin{equation}
    P(y = i \mid \mathbf{z}) = \frac{\exp(z_i)}{\sum_{j=1}^{K} \exp(z_j)}
\end{equation}

Here, \( \exp(z_i) \) ensures that all values are positive, and dividing by the sum normalizes the probabilities so that they sum to 1. This normalization is crucial for classification, as it allows the network's outputs to represent the likelihood of each class.

The \textit{Cross-Entropy loss} measures the difference between the predicted probability distribution \( \mathbf{P} \) (produced by Softmax) and the true distribution \( \mathbf{y} \) (the one-hot encoded ground truth). It is defined as:

\begin{equation}
    \mathcal{L}_{\text{CE}} = -\sum_{i=1}^{K} y_i \log(P(y = i \mid \mathbf{z}))
\end{equation}


For a single example where the true class is \( c \), this simplifies to:

\begin{equation}
    \mathcal{L}_{\text{CE}} = -\log(P(y = c \mid \mathbf{z}))
\end{equation}


This formulation penalizes incorrect predictions by heavily weighting the log of the predicted probability for the true class. The loss is minimized when the predicted probability \( P(y = c \mid \mathbf{z}) \) approaches 1, indicating high confidence in the correct class.

This combination has become the de facto standard for image classification tasks, providing a robust and mathematically sound framework for training deep neural networks.


\myindent \textbf{Weighted Softmax Cross-Entropy Loss}
The \textit{Weighted Softmax Cross-Entropy loss}, often denoted as \textit{weighted softmax loss}, is a variant of the standard Softmax-Cross-Entropy loss, designed to address imbalanced datasets. By assigning different weights to each class, this method ensures that underrepresented classes contribute more to the overall loss, improving the model's performance on minority classes. The weighted cross-entropy loss applies class-specific weights to the standard Cross-Entropy formulation. It is defined as:

\begin{equation}
    \mathcal{L}_{\text{WCE}} = -\sum_{i=1}^{K} w_i y_i \log(P(y = i \mid \mathbf{z}))
\end{equation}

Where \( w_i \) is the weight for class \( i \), reflecting its relative importance, \( y_i \) is the one-hot encoded true label for class \( i \), and \( P(y = i \mid \mathbf{z}) \) is the predicted probability for class \( i \) (produced by the Softmax function).

For a single example where the true class is \( c \), the loss simplifies to:

\begin{equation}
    \mathcal{L}_{\text{WCE}} = -w_c \log(P(y = c \mid \mathbf{z}))
\end{equation}

This weighted formulation ensures that minority classes (with higher weights) contribute more to the overall loss, addressing the imbalance during training and improving the model's performance on underrepresented classes.

\myindent \textbf{Focal Loss}

\myindent \textbf{Class-Balanced Loss}

\myindent \textbf{Balanced Softmax Loss}

\myindent \textbf{LDAM Loss}

\myindent \textbf{Equalization Loss}


\subsection{Information Augmentation}
Data augmentation techniques tailored for long-tailed datasets.

\subsubsection{Transfer Learning}

\subsubsection{Data Augmentation}



\subsection{Module Improvement}
Architectural changes to improve tail-class representation.