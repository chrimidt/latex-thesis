% Chapter 2: Background

This chapter presents the different background topics of the thesis work, which are the long-tailed datasets, model architectures \textit{Convolutional Neural Networks (CNN)} and 
\textit{Visual Transformers (VT)}, the deep long-tailed learning methods \textit{Class Re-balancing (CR)}, \textit{Information Augmentation (IA)}, 
and \textit{Module Improvement (MI)}. These topics will be explained for the reader.

\section{Long-Tailed Dataset}
A short introductory paragraph explaining why these topics are relevant and how they tie into the thesis.
Contextualize the sectionsâ€”e.g., "Long-tailed datasets are central to this thesis, as they represent the primary challenge. Model architectures and classic long-tailed methods form the foundation of the approaches explored in this work."

\section{Model Architechtures}
Briefly describe the role of deep learning models in handling long-tailed datasets.

\subsection{Convolutional Neural Networks}
Add historical context (e.g., their success in computer vision tasks).
Highlight specific CNNs used in this thesis (e.g., ResNet, MobileNet).

\subsection{Visual Transformers}
Explain their advantages over CNNs for certain tasks.
Mention why they are relevant for handling long-tailed datasets.


\section{Classic Long-Tailed Methods}
Introduce the three methods (CR, IA, MI) with a brief explanation of their purpose.

\subsection{Class Re-balancing}
Techniques like oversampling/undersampling and class weighting. Loss functions.

\subsection{Information Augmentation}
Data augmentation techniques tailored for long-tailed datasets.

\subsection{Module Improvement}
Architectural changes to improve tail-class representation.