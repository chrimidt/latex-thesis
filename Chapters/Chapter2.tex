% Chapter 2: Background

% Length: aim for 10-15 pages

This chapter presents the different background topics of the thesis work, which are the long-tailed datasets, model architectures \textit{Convolutional Neural Networks (CNN)} and \textit{Visual Transformers (VT)}, the deep long-tailed learning methods \textit{Class Re-balancing (CR)}, \textit{Information Augmentation (IA)}, 
and \textit{Module Improvement (MI)}. These topics will be explained for the reader.\\

Maybe mention image classification, as it is the primary goal of this thesis. 

\section{Long-Tailed Dataset}
A short introductory paragraph explaining why these topics are relevant and how they tie into the thesis.
Contextualize the sectionsâ€”e.g., "Long-tailed datasets are central to this thesis, as they represent the primary challenge. Model architectures and classic long-tailed methods form the foundation of the approaches explored in this work."\\

Describe the difference between class-imbalanced learning and long-tailed learning.

\section{Model Architechtures}
Briefly describe the role of deep learning models in handling long-tailed datasets.

\subsection{Convolutional Neural Networks}
Add historical context (e.g., their success in computer vision tasks).
Highlight specific CNNs used in this thesis (e.g., ResNet, MobileNet).

\subsection{Visual Transformers}
Explain their advantages over CNNs for certain tasks.
Mention why they are relevant for handling long-tailed datasets.


\section{Classic Long-Tailed Methods}
Introduce the three methods (CR, IA, MI) with a brief explanation of their purpose.\\

Following the paper \textit{Deep Long-Tailed Learning: A Survey} \cite{zhang2023deep}, the existing deep long-tailed learning methods are grouped into three main categories based on their technical approach: class re-balancing, information augmentation, and module improvement. These categories are further divided onto sub-categories: re-sampling, class-sensitive learning, logit adjustment, transfer learning, data augmentation, representation learning, classifier desing, decoupled training, and ensemble learning (TODO: create a figure like figure 2 in the paper). This thesis does not aim to examine all the beforementioned method, but aims to find a deep learning approach to a specific problem. The backgrounds of the methods used in this thesis are described in this section.

\subsection{Class Re-balancing}
The class re-balancing method aims to re-balance the effect of the imbalanced training dataset, and has three main sub-categories: re-sampling, class-sensitive learning, and logit adjustment \cite{zhang2023deep}. 

\subsubsection{Re-sampling}
The traditional way to sample when training deep networks is bases on mini-batch gradient descent with random sampling. This means that each sample has an equal probability of being sampled. When sampling from an imbalanced dataset, samples from head classes naturally occur more often, and thus have higher chance of being sampled than samples from tail classes, making the resulting deep models biased towards head classes. Re-sampling is a method that adresses this problem by adjusting the number of samples per class in each sample batch for model training. 

\subsubsection{Class-sensitive Learning}
Class-sensitive learning incorporates strategies to adjust the loss function, making it more sensitive to the imbalanced nature of the dataset. This approach directly modifies the optimization process to prioritize learning from under-represented tail classes.

\subsubsection{Loss Functions for Class-Sensitive Learning}
Re-weighting  is a method that attempts to adjust the training loss values for different classes by multiplying them with different weights \cite{zhang2023deep}.

\myindent \textbf{Cross-Entropy Loss}
Softmax loss is the standard loss function used for classification tasks. It converts logits into probabilities via the softmax function and calculates the cross-entropy between predicted probabilities and ground-truth labels. While effective for balanced datasets, softmax loss struggles with long-tailed datasets because it assumes equal importance for all classes, leading to biased predictions favoring head classes.

\myindent \textbf{Weighted Softmax Loss}

\myindent \textbf{Focal Loss}

\myindent \textbf{Class-Balanced Loss}

\myindent \textbf{Balanced Softmax Loss}

\myindent \textbf{LDAM Loss}

\myindent \textbf{Equalization Loss}


\subsection{Information Augmentation}
Data augmentation techniques tailored for long-tailed datasets.

\subsubsection{Transfer Learning}

\subsubsection{Data Augmentation}



\subsection{Module Improvement}
Architectural changes to improve tail-class representation.