% Chapter 2: Background

% Length: aim for 10-15 pages

This chapter presents the different background topics of the thesis work, which are the long-tailed datasets, model architectures \textit{Convolutional Neural Networks (CNN)} and \textit{Vision Transformers (VT)}, the deep long-tailed learning methods \textit{Class Re-balancing (CR)}, \textit{Information Augmentation (IA)}, 
and \textit{Module Improvement (MI)}. These topics will be explained for the reader.

\todo{Mention image classification, as it is the primary goal of this thesis.} 

% ===============================================================================
% LT Datasets

\section{Long-Tailed Datasets}
\label{sec:lt-datasets}
Long-tailed datasets pose significant challenges in deep learning, as they represent an extreme form of class imbalance. Addressing these challenges is central to this thesis, which explores methods to improve model performance on underrepresented classes. This section outlines the structure of long-tailed distributions and their implications.

A balanced dataset is one where all classes are evenly represented, whereas imbalanced datasets feature varying sample sizes across classes. Long-tailed datasets are characterized by a significant class imbalance, where a few dominant classes account for most samples (head classes), while the majority of classes are underrepresented (tail classes) as depicted in Figure \ref{fig:lt_distribution}. This  distribution is common for real-world datasets \cite{Newman_2005, liu2019largescalelongtailedrecognitionopen}. For example, the iNaturalist, a popular benchmark for image classification, exhibits a long-tailed distribution of species \cite{vanhorn2018inaturalistspeciesclassificationdetection}. Other benchmarks are constructed by sampling from datasets such as ImageNet \cite{ImageNet2009} by using a Pareto distribution, which simulates long-tailed class distributions with a power-law decay \cite{zhang2023deep, dealvis2024surveydeeplongtailclassification,cao2019learningimbalanceddatasetslabeldistributionaware}.

CIFAR100-LT \cite{cao2019learningimbalanceddatasetslabeldistributionaware}, derived from the CIFAR-100 dataset \cite{krizhevsky2009learning}, serves as the primary dataset for the experiments conducted in this thesis. CIFAR-100 is a widely used benchmark in classification research due to its diverse class representation and manageable size. It consists of 60,000 32x32 color images divided into 100 classes, each with 600 samples. These are further split into 500 training images and 100 testing images per class. CIFAR100-LT is created by reducing the number of samples in certain classes of CIFAR-100 following an exponential decay en sample sizes, given by:

\begin{equation}
    \label{eq:exp}
    n_i = n_{max}\cdot \text{IR}^{\frac{i-1}{C-1}}
\end{equation}

Where $n_i$ is the number of samples in class $i$, $n_{max}$ is the number of samples in the most frequent class, IR is the imbalance ratio, and $C$ is the total number of classes \cite{kaidic_ldam_drw}.

Other long-tailed datasets follow a Pareto distribution with number of samplers per class as followed \cite{liu2019largescalelongtailedrecognitionopen}:

\begin{equation}
    \label{eq:pareto}
    f(C) = \frac{\alpha C_{\text{m}}^\alpha}{C^{\alpha + 1}}, \quad C \geq C_{\text{m}}, \quad \alpha > 0
\end{equation}

Here, $\alpha$ is the shape parameter, $C_{\text{m}}$ is the scale parameter representing the minimum possible class index, and $C$ is the class index. A larger $\alpha$ results in a more severe imbalance.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{Images/long_tail_distribution.png} 
    \caption{Illustration of a long-tailed distribution. Figure from \cite{lgresearch257}.}
    \label{fig:lt_distribution} % Use a unique label for referencing the figure
\end{figure}

Class imbalance has a profound impact on model performance compared to evenly distributed datasets \cite{vanhorn2017deviltailsfinegrainedclassification, cui2019classbalancedlossbasedeffective}. Deep networks trained on long-tailed datasets often exhibit biased performance, favoring head classes while performing poorly on tail classes \cite{zhang2023deep}. Zhang et al. (2023) provide a comprehensive survey of methods addressing this challenge, categorizing current approaches into three main groups: class re-balancing, information augmentation, and module improvement. These methods will be further explored in section \ref{sec:lt_methods}. 

\todo{Write something of the following:}
"Let $\{x_i, y_i\}_{i=1}^n$ be the long-tailed training set, where each sample $x_i$ has a corresponding class label $y_i$. The total number of training samples over $K$ classes is $n = \sum_{k=1}^K n_k$, where $n_k$ denotes the number of samples in class $k$. Let $\pi$ denote the vector of label frequencies, where $\pi_k = \frac{n_k}{n}$ indicates the label frequency of class $k$. Without loss of generality, a common assumption in long-tailed learning \cite{31, 32} is that the classes are sorted by cardinality in decreasing order (i.e., if $i_1 < i_2$, then $n_{i_1} \geq n_{i_2}$, and $n_1 \gg n_K$). The imbalance ratio is then defined as $\frac{n_1}{n_K}$." \cite{zhang2023deep}


% ===============================================================================
% Model architechtures

\section{Model Architecures}
\label{sec:model_arch}
Deep learning has revolutionized image classification by introducing models capable of learning complex patterns and representations from data. Among these, Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) are chosen as the primary architectures used in this thesis due to their performance on image classification tasks. This section provides a theoretical foundation for these models, focusing on the specific architectures utilized: MobileNetV2 \cite{sandler2018mobilenetv2}, ResNet50V2 \cite{he2015deepresiduallearningimage}, and ConvNeXt Base \cite{todi2023convnext} as the CNN architectures, and ViT-B/16 \cite{dosovitskiy2021imageworth16x16words} as the ViT architecture.
These models were chosen to represent a spectrum of design paradigms; from the traditional ResNet to lightweight architectures and transformers. By evaluating models of varying complexity, this study aims to identify architectures best suited for long-tailed learning under different resource constraints and data distributions.

% ===============================================================================
% DNNs

\subsection{Introduction to Deep Neural Networks}
\label{sec:intro_DNN}
Before the introduction of Convolutional Neural Networks (CNNs) and, more recently, Vision Transformers (ViTs), the standard approach for image classification involved flattening a two-dimensional image matrix into a one-dimensional array and passing it through a Multilayer Perceptron (MLP), also known as a feed-forward neural network. MLPs are fully connected neural networks composed of an input layer, output layer, and one or more hidden layers. Being fully connected means that each neuron in a given layer is connected to all neurons in the next layer, forming a dense network. These connections are associated with weights and biases, which the network learns during training. Input features are fed into the input layer, propagated through hidden layers that add complexity to model nonlinear relationships, and yield predictions in the output layer. Known as universal approximators, MLPs can approximate any continuous function given sufficient neurons in the hidden layers \cite{zhang2023dive,HORNIK1989359}.

To illustrate the structure of a neural network, figure \ref{fig:dnn_layers} shows an example of a feed-forward neural network with three input neurons, two hidden layers, each with four neurons, and two output neurons. This architecture could be used, for instance, to classify images of cats and dogs based on three input features, such as height, weight, and width of the animals. The input propagates through the network, with each neuron computing a weighted sum of its inputs followed by an optional nonlinearity. The final output is a prediction, where the class corresponding to the neuron with the highest value is chosen.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{Images/cnn_layers.jpg} 
    \caption{Layers of a neural network. Figure from \cite{mathworks_cnn}. \todo{Make this figure.}}
    \label{fig:dnn_layers}
\end{figure}

However, this simple neural network becomes insufficient for more complex problems, such as image classification, as it requires an increasing number of parameters. For instance, a $224\times 224$ RGB image flattened is very large, making MLPs parameter-heavy and inefficient. The limitations of MLPs were addressed by Convolutional Neural Networks (CNNs), which introduced convolutional and pooling layers to effectively preserve and utilize the spatial information of pixels in two-dimensional images \cite{zhang2023dive}.


% ===============================================================================
% CNNs

\subsection{Convolutional Neural Networks}
\label{sec:CNNs}
Convolutional Neural Networks (CNNs) \cite{lecun1995} were introduced to address the limitations of MLPs for image-related tasks. Unlike MLPs, which treat input features as independent, CNNs are designed to recognize patterns in images by applying local filters through convolutional layers, and thereby preserving the two dimensional input of an image \cite{lecun1998,NIPS2012_c399862d,zhang2023dive}. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{Images/CNN_illustration.jpg} 
    \caption{Illustration of a convolutional neural network. Figure from \cite{mathworks_cnn}. \todo{Make this figure.}}
    \label{fig:cnn_illustration}
\end{figure}

CNNs consist of several core components, as illustrated in Figure~\ref{fig:cnn_illustration}, and have three main types of layers: convolutional layers, pooling layers, and a fully connected layer \cite{cs231n}. The convolutional layer is the first layer, and serves to extract local features by applying filters to small regions of an image. Pooling layers reduce the spatial dimensions of feature maps, providing invariance to small translations. CNNs are typically made of multiple convolution and pooling layers, but the final layer is the fully connected layer. Activation functions introduce nonlinearity, allowing the network to capture complex patterns. At the final stage, fully connected or global pooling layers aggregate the extracted features into predictions, enabling tasks such as classification or segmentation. CNNs use three-dimensional data for image classification: \emph{width} \emph{height}, \emph{depth}. As an example, an image from the CIFAR-100 dataset $32\times 32\times 3$, corresponding to width, height, and 3 color channels (RGB). The following provides an overview of function of the layers:

\paragraph{Convolutional Layer}
The convolutional layer is the foundation of CNNs. It applies a set of filters (called kernels) to the input image, performing convolutions to produce feature maps \cite{cs231n}. Each filter is designed to detect specific features, such as edges or textures. The filter, which has smaller dimensions than the input, is applied to an area of the image, computing dot products between the weights of the filter and the input pixels, as illustrated in figure \todo{make this figure}. Afterwards, the filter shifts by a stride, repeating the process until the kernel has computed the dot products for entire image. This process enables the network to learn spatial hierarchies of features, with the early layers capturing patterns like edges, and later layers capturing detailed structures \todo{reference}. After each convolution, the ReLu transformation is applied to introduce non-linearity \cite{cs231n}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{example-image-a} 
    \caption{Illustration of a convolution. \todo{Make this figure.}}
    \label{fig:convolution}
\end{figure}

\paragraph{Pooling Layer}
The function of the pooling layer (also known as downsampling) is to reduce the spatial dimensions of the feature maps, decreasing the number of parameters and computational load of the netwrok \cite{cs231n}. Moreover, this contributes to making the detection of features invariant to scale and orientation.  

\paragraph{Fully Connected (FC) Layer}
The FC layer, as the name implies, connects every neuron in one layer to every neuron in the next. Positioned at the end of the network, this performs the task of classification based on the features extracted from the previous layers \cite{cs231n}.
\vspace{1em}

\noindent These layers, combined with the architectural design of CNNs, introduce inductive biases that make them particularly well-suited for image-related tasks \cite{kim2020inductivebias}. 
% For instance, the use of convolutional filters enforces local connectivity, enabling the network to focus on localized patterns such as edges and textures. Additionally, the weight-sharing mechanism in convolutional layers provides translation invariance, allowing features to be recognized regardless of their position within the image.

% Pooling layers contribute to a spatial hierarchy, making the network robust to small positional changes and reducing computational complexity. Moreover, the multi-layered structure of CNNs facilitates hierarchical feature learning, where early layers capture simple features, and deeper layers learn increasingly complex representations. These inductive biases are fundamental to the effectiveness of CNNs in tasks like image classification and segmentation, as they align closely with the structure and characteristics of image data.





CNNs gained popularity after the introduction of LeNet-5 by LeCun et al. in 1998 \cite{lecun1998}, which showcased their capability to recognize handwritten digits. The field advanced significantly in 2012 when AlexNet \cite{NIPS2012_c399862d} won the ImageNet Challenge, demonstrating the effectiveness of deeper architectures and multi-GPU training for large-scale image recognition tasks. Subsequent developments led to influential architectures such as VGGNet \cite{simonyan2015deepconvolutionalnetworkslargescale}, GoogLeNet \cite{szegedy2014goingdeeperconvolutions}, and the state-of-the-art ResNet \cite{he2015deepresiduallearningimage}. The CNN architectures explored in this thesis are detailed below.

% ===============================================================================
% ResNet50

\subsubsection{ResNet50 Architecture}
\label{sec:resnet}
ResNet50 is a variant of the Residual Network (ResNet) architecture, developed by Microsoft Research (He et al.) in 2015 \cite{he2015deepresiduallearningimage}. The ResNet architecture was designed to address the vanishing and exploding gradient problem in deep networks. When training a deep neural network, as the number of layers increases, the gradients of the loss function with respect to the weight can potentially vanish or explode during backpropagation \cite{he2015deepresiduallearningimage}. This can lead to slow convergence or unstable updates.

In traditional CNNs, stacked layers learn a direct mapping $\mathcal{H}(x)$ of the input $x$ to the output. The introduction of residual layers, however, allows for the layer to fit a residual mapping, as illustrated in figure \ref{fig:res_learning}, where the network learns the residual function $\mathcal{F}(x) = \mathcal{H}(x) - X$. Isolating $\mathcal{H}(x)$ yields $\mathcal{H}(x) = \mathcal{F}(x) + x$, meaning that the input $x$ is passed through a skip connection. This architecure reduces the complexity of the optimization process, as the network only has to model the residual component $\mathcal{F}(x)$ rather than the full mapping $\mathcal{H}(x)$ \cite{he2015deepresiduallearningimage}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{Images/res_learn.png} 
    \caption{Residual Learning. Figure from \cite{he2015deepresiduallearningimage}. \todo{Make this figure.}}
    \label{fig:res_learning}
\end{figure}

The residual network is formed by stacking multiple layers of residual blocks, as the one depicted in figure \ref{fig:res_learning}. The ResNet architecture can have a varying number of layers, and, as the name implies, the ResNet50 variant has 50 layers. Other architecures include ResNet-34, ResNet-101, and ResNet-152 \cite{he2016identitymappingsdeepresidual}. 

The ResNet50 implementation consist of 1 convolutional layer, 4 stages of bottleneck residual blocks with layers [3, 4, 6, 3], global average pooling, and finally a fully connected layer for classification\cite{torchvision-resnet}, as seen in figure \ref{fig:resnet50}. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{Images/resnet50.png} 
    \caption{ResNet50 Architecture. Figure from \url{https://towardsdatascience.com/the-annotated-resnet-50-a6c536034758}. \todo{Make this figure.}}
    \label{fig:resnet50}
\end{figure}

% ===============================================================================
% MobileNetV2

\subsubsection{MobileNetV2 Architecture}
\label{sec:mobilenet}
MobileNetV2 introduced by Sandler et al. \cite{sandler2018mobilenetv2} is a lightweight CNN model designed primarily to balance model accuracy and computational efficiency, making it suitable for mobile or embedded devices. Building upon the original concepts of MobileNetV1 \cite{howard2017mobilenetsefficientconvolutionalneural}, MobileNetV2 preserves the use of depthwise separable convolutions, a method for reducing the parameters and floating-point operations, while introducing a novel element known as the inverted residual structure. 

% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.8\textwidth]{Images/mobilenet_structure.png} 
%     \caption{Illustration of the MobileNetV1 structure. Figure from \cite{howard2017mobilenetsefficientconvolutionalneural}. \todo{Make this figure.}}
%     \label{fig:MobileNetV1_structure}
% \end{figure}

\paragraph{Inverted Residual Blocks}
While traditional residual connections described above allow for identity mapping and improved gradient flow, MobileNetV2 employs an inverted residual structure \cite{sandler2018mobilenetv2}. Instead of mapping from a high-dimensional representation down to a lower-dimensional bottleneck, then reconstructing features at the output, inverted residual blocks begin with a low-dimensional input and expand it to a higher-dimensional space before applying a depthwise convolution. After spatial filtering, the representation is projected back down to a low-dimensional space. This approach, illustrated in Figure \ref{fig:residual}, helps preserve crucial information and maintain a rich feature space without substantially increasing computational cost. The use of a linear bottleneck (i.e., no nonlinear activation in the low-dimensional projection) also helps prevent the destruction of useful information, further improving efficiency and accuracy.  

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{Images/inverted_residual.png} 
    \caption{Residual and Inverted Residual Blocks. Figure from \cite{sandler2018mobilenetv2}. \todo{Make this figure.}}
    \label{fig:residual}
\end{figure}

\paragraph{Depthwise Separable Convolution}
Following MobileNetV1, MobileNetV2 relies on depthwise separable convolutions to factorize the convolution operation into two simpler operations \cite{howard2017mobilenetsefficientconvolutionalneural}: depthwise convolution and pointwise convolution as shown in figure \ref{fig:depthwise_sep_conv}. The depthwise convolution applies a single filter to each input channel, and the pointwise convolution (a $1\times 1$ convolution) then recombines the channels to produce the desired output. In comparison, a standard convolution both filtes and combines inputs into a new set of outputs. This approach reduces the parameter count and computational load, making the model suitable for devices with limited resources \cite{howard2017mobilenetsefficientconvolutionalneural,sandler2018mobilenetv2}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{Images/depthwise_separable_conv.jpg} 
    \caption{Depthwise Separable Convolution. Figure from \url{https://www.sciencedirect.com/topics/computer-science/depthwise-separable-convolution}. \todo{Make this figure.}}
    \label{fig:depthwise_sep_conv}
\end{figure}




% ===============================================================================
% Vision Transformers

\subsection{Vision Transformers}
\label{sec:ViTs}
\todo{Explain their advantages over CNNs for certain tasks.
Mention why they are relevant for handling long-tailed datasets.}

Transformers were introduced by Vaswani et al. in 2017 \cite{vaswani2023attentionneed}, and revolutionized the deep learning field, surpassing Recurrent Neural Networks (RNNs) for Natural Language Processing (NLP) tasks  \cite{v7labs-vit,vaswani2023attentionneed}. The design behind transformers is based on self-attention mechanism, allowing the model to weigh the significance of each part of input. As their design lack recurrence or convolutions, transformers use positional embeddings to represent the order of tokens in a sequence \cite{vaswani2023attentionneed}. Later, the Vision Transformer (ViT), an adaptation of the transformer architecture for image processing, was introduced by Dosovitskiy et al. (2021) \cite{dosovitskiy2021imageworth16x16words}. Here, images are represented as sequences including the class label as a learnable token for classification. The input image is divided into a sequence of patches, which are then flattened and liniarly embedded into a vector. The spacial information is preserved by adding positional encodings to the embeddings. Next, the sequence is fed into a transformer encoder identical to that introduced by Vaswani et al., consisting of alternating layers of Multi-head self-attention (MSP) and Multi-Layer Perceptron (MLP) blocks with Layer Norm (LN) applied before every block, and residual connection after every block. The final MLP layer acts as the classification head \cite{dosovitskiy2021imageworth16x16words}. The illustration in figure \ref{fig:vit_arch} shows the architecture of Vision Transformers.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{Images/vit.png} 
    \caption{Vision Transformer architecture. Figure from \cite{dosovitskiy2021imageworth16x16words}. \todo{Make this figure.}}
    \label{fig:vit_arch}
\end{figure}

Unlike CNNs, ViTs have much less image-specific inductive bias \cite{dosovitskiy2021imageworth16x16words}. While the inductive bias in CNNs help the model generalize to unseen data \cite{kim2020inductivebias}, the ViT architecture does not include built-in assumptions about the structure of the data, only the MLP layers are local an translationally equivalent \cite{dosovitskiy2021imageworth16x16words}. As a result, ViTs require more training data to learn spacial relations compared with CNNs.

\todo{What made the ViT get SOTA? Why were they better than CNNs? Scalability. \url{https://www.youtube.com/watch?v=QqejV0LNDHA}.}

% ===============================================================================
% ViT-B/16

\subsubsection{ViT-B/16 Architecture}
\label{sec:vitb16}
the ViT-B/16 is a Vision Transformer that leverages the transformer architecture for image classification \cite{dosovitskiy2021imageworth16x16words}, pre-trained on \todo{ImageNet-1K or ImageNet-21K?}. One of the main characteristics of the ViT-B/16 is that the input image consist of a fixed $16 \times 16$ patch size. It has 12 stacked encoder layers, each computing self-attention with 12 heads \cite{dosovitskiy2021imageworth16x16words}. Additionally, each layer includes an MLP, and skip-connections are applied around self-attention and MLP blocks for better gradient flow. LN is applied before the attention and MLP blocks. Afterwards, the appended class token is extracted, and a fully connected layer maps the token to the class prediction \cite{torchvision2024vitb16}. 


% ===============================================================================
% ConvNeXt Base

\subsubsection{ConvNeXt Base Architecture}
\label{sec:convnext}
ConvNext, introduced by Liu et al. in 2022 \cite{liu2022convnet2020s}, evolutionized the traditional CNN architecture by incorporating elements from ViTs. Starting with the stem cell, the ConvNeXts employ a patchify stem, as seen in figure \ref{fig:stem}. By setting the kernel to $4 \times 4$ and the stride to 4, the result is a non-overlapping convolution where no information is shared between them. However, these are later combined in the final layers of the network.  

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.6\textwidth]{Images/stem.png} 
    \caption{ResNet stem vs ConvNeXt stem. Figure from \url{https://www.kungfu.ai/blog-post/convnext-a-transformer-inspired-cnn-architecture} \todo{Make this figure.}}
    \label{fig:stem}
\end{figure}

Next, inspired by ResNeXt \cite{xie2017aggregatedresidualtransformationsdeep}, the ConvNeXt use depthwise convolution, already described in the MobileNetV2 section \ref{sec:mobilenet}, to increase the FLOPs/accuracy trade-off. Likewise, the ConvNext incorporates inverted bottleneck block, which are an important design in transformers as well as MobileNetV2 \cite{liu2022convnet2020s}. Additionally, the ConvNext architecure uses a $7 time 7$ depthwise convultion in each block on the condition of moving the depthwise convolution layer up. See figure \ref{fig:conv_block}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.6\textwidth]{Images/convnext_block_mod.png} 
    \caption{Block modifications. Figure from \cite{liu2022convnet2020s} \todo{Make this figure.}}
    \label{fig:conv_block}
\end{figure}

Instead of using the commonly used activation function in ResNets, the Rectified Linear Unit (ReLU), the ConvNeXt use Gaussian Error Linear Unit (GeLU). This activation function has shown to perform better than ReLU for transformers \cite{liu2022convnet2020s}. Additionally, ConvNext draws inspiration from transformers by removing most instances of normalization layers, only leaving them after depthwise convolutions and furhter replacing the Batch Normalization with Layer Normalization. Finally, ConvNeXt introduces separate downsampling layers between blocks rather than implementing downsampling within the blocks themselves. 



% \subsection{Transfer Learning}
% In this thesis, transfer learning is used to 

% Transfer learning involves training a model on data from a source domain 200 TS = P(y|X) and then transferring it to a target domain TT = P(y|X) – typically with less data available. 

%====================================================================================
% Long-Tailed Methods

\section{Class Re-balancing Methods for Long-Tailed Learning}
\label{sec:lt_methods}

Class re-balancing methods in deep learning seeks to mitigate the effects of imbalanced class distributions in training data. According to Zhang et al. \cite{zhang2023deep} class re-blancing can be divided into three sub-categories: re-sampling, class-sensitive learning, and logit adjustment. In this thesis, the primary focus is on class-sensitive learning, exploring re-weighting, re-margining, and logit modification strategies. For completeness, re-sampling is briefly introduced as well.


\subsection{Re-Sampling}
\label{sec:re-sampling}
Deep networks are commonly trained with mini-batch gradient descent using random sampling, i.e. each sample has an equal probability of being selected \cite{zhang2023deep}. When classes are imblanced, samples from head classes naturally occur more often, thus having higher chances of being selected than samples from tail classes, resulting in a bias towards head classes. Re-sampling adresseses this problem by adjusting the sample probabilites or the sample counts per class (e.g. oversampling minority classes or undersampling majority classes). While this approach seems simple and intuitive, re-sampling can lead to overfitting of tail classes or underperformance on head classes \cite{zhang2023deep}, hence the need for more sophistacated approaches.

\subsection{Class-Sensitive Learning}
\label{sec:class-sensitive-learning}

Traditional training methods using the standard loss function, cross-entropy loss, can lead the model to be biased towards head classes, as the loss ignores class imbalance and thus generate an uneven amount of gradients for different classes \cite{zhang2023deep}, as described later in this section. Consequently, tail classes are often misclassified. To address this imbalance, class-sensitive learning methods modify the loss function \todo{or how losses er calculated} to pay more attention to minority classes, thus improving overall performance.

The three sub-categories of class-sensitive learning explored in this thesis are described in the following.

\paragraph{Re-Weighting}
Modifies the loss function, commonly the cross-entropy loss, by assigning different weights to each class \cite{zhang2023deep}. Classes with fewer samples are assigned greater weights, thus re-balancing the training process.

\paragraph{Re-Margining}
Adjusts the decision boundaries by introducing class-dependent margins, ensuring that minority classes have more room to establish discriminative features in the embedding space \cite{zhang2023deep, cao2019learningimbalanceddatasetslabeldistributionaware}. 

\paragraph{Logit Adjustment}
Directly shifts the model's raw outputs using class priors \cite{ren2020balancedmetasoftmaxlongtailedvisual,menon2021longtaillearninglogitadjustment}. This approach normalizes the logits to account for imbalance in class distribution before computing the loss. 
\vspace{1em}

In the following section, the theory behind several representative loss functions commonly used in class-sensitive learning is presented, beginning with an introduction to loss functions followed by the baseline Softmax Cross-Entropy loss. Subsequently, the theory behind multiple re-weighting schemes including Weighted Softmax Cross-Entropy, Focal Loss, Class-Balanced Loss, Balanced Softmax Loss, and Equalization Loss, as well as the re-margining method LDAM Loss. Each of these methods seeks to improve the imbalance in training through modifications that ultimately improve the classification performance on tail classes.

\section{Loss Functions}
\label{sec:intro_losses}
Loss functions are a fundamental component in deep learning, as they serve as a measure of how far model predictions deviate from the actual values \cite{zhang2023dive,Goodfellow-et-al-2016}.

During training, an optimization algorithm iteratively updates the model parameters (weights and biases) with the goal of minimizing the loss using the gradient of the loss function with respect to each parameter to determine how the parameter affects the loss \cite{Goodfellow-et-al-2016}. Consequently, the loss function influences the model's interpretation of prediction errors and guides parameter updates. This process continues until the model converges or meets a stopping criterion. The experiments in this thesis are conducted using the Adam optimizer introduced by Kingma et al. \cite{kingma2017adammethodstochasticoptimization}.

For image classification tasks, the standard loss function is the cross-entropy loss combined with the softmax activation function \cite{zhang2023dive} described in the following.
% Afterwards, the model update its parameter to reduce the loss, following:

% \begin{equation}
%     \label{eq:gradient_update}
%     w_{t+1} = w_t - \eta \frac{\partial L}{\partial w_t}
% \end{equation}

% Here, $w$ are the parameters, $\eta$ is the learning rate, and $L$ is the loss function. This is an iterative process, and continues until the loss reaches a minimum or the model converges.

% From equation \eqref{eq:gradient_update}, 


% \todo{This is the SGD. Include the Adam Optimizer.}
% The Adam optimizer introduced by Kingma et al. \cite{kingma2017adammethodstochasticoptimization} 
% Adam is a replacement optimization algorithm for stochastic gradient descent for training deep learning models.
% Adam combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems.
% Adam is relatively easy to configure where the default configuration parameters do well on most problems.

\subsection{Softmax Cross-Entropy Loss}
The \emph{Softmax Cross-Entropy (CE) loss} is a fundamental building block in training deep classifiers and is widely regarded as the baseline in classification tasks \cite{zhang2023deep, cs231n, pytorch_crossentropy}. The softmax loss, as it is commonly known as, is the combination of the cross-entropy loss and the softmax activation function \cite{zhang2023dive,Goodfellow-et-al-2016}. First, the need of the softmax activation function will be decribed.

The \textit{Softmax} function transforms the raw output scores (logits) of the final layer of a neural network into a probability distribution over \( K \) classes. For an input \( \mathbf{z} = [z_1, z_2, \dots, z_K] \), the Softmax function for class \( i \) is defined as:

\begin{equation}
    P(y = i \mid \mathbf{z}) = \frac{\exp(z_i)}{\sum_{j=1}^{K} \exp(z_j)}
\end{equation}

Here, \( \exp(z_i) \) ensures that all values are positive, and dividing by the sum normalizes the probabilities so that they sum to 1. This normalization is crucial for classification, as it allows the network's outputs to represent the likelihood of each class.

The \textit{Cross-Entropy loss} measures the difference between the predicted probability distribution \( \mathbf{P} \) (produced by Softmax) and the true distribution \( \mathbf{y} \) (the one-hot encoded ground truth). It is defined as:

\begin{equation}
    \mathcal{L}_{\text{CE}} = -\sum_{i=1}^{K} y_i \log(P(y = i \mid \mathbf{z}))
\end{equation}

For a single example where the true class is \( c \), this simplifies to:

\begin{equation}
    \label{eq:ce_loss}
    \mathcal{L}_{\text{CE}} = -\log(p_y)
\end{equation}

This equation penalizes incorrect predictions by heavily weighting the log of the predicted probability for the true class. The loss is minimized when the predicted probability \( P(y = c \mid \mathbf{z}) \) approaches 1, indicating high confidence in the correct class.

While this approach provides a robust and stable training objective, it treats all classes equally and does not account for class imbalance.

\paragraph{Bias}
The standard Softmax Loss (eq. \eqref{eq:ce_loss}) estimates class probabilities based on the assumption that the training and validation labels follow the same distribution, $p(y=j)$, in other words how common is the label $j$ in the data \cite{ren2020balancedmetasoftmaxlongtailedvisual}. When training with an imbalanced dataset the estimated probabilities $\phi$ can become biased when applied to data with a different class distribution, e.g. a balanced test set, which can lead to errors in predictions.

% \subsubsection{Summary of Class Re-Balancing}
% Long-tailed or imbalanced datasets pose significant challenges in classification tasks, as models trained naively tend to bias toward majority classes. Class re-balancing strategies tackle this by either sampling differently (re-sampling), adjusting the loss function to emphasize minority classes (cost-sensitive re-weighting and margin-based losses), or modifying logits based on class priors (logit adjustment). These techniques all aim to improve performance on tail classes without excessively sacrificing performance on head classes. The following sections detail how each method is applied and evaluated in the context of this thesis.

\subsubsection{Re-Weighting}
\label{sec:re-weighting}
\paragraph{Weighted Softmax Cross-Entropy Loss}
The \emph{Weighted Softmax Cross-Entropy (WCE) loss} modifies the standard CE loss (equation \eqref{eq:ce_loss}) by introducing a weighting factor $w$. For multiclass classification, the CE loss it is multiplied by the inverse class frequencies $1/\pi_y$ \cite{zhang2023deep,lin2018focallossdenseobject}, given by:

\begin{equation}
    \label{eq:wce_loss}
    \mathcal{L}_{\text{WCE}} = -w \log(p_y) = - \frac{1}{\pi_y} \log(p_y)
\end{equation}

This weighted CE loss ensures that minority classes contribute more to the overall loss, addressing the imbalance during training.


\paragraph{Focal Loss}
\emph{Focal Loss} \cite{lin2018focallossdenseobject} addresses class imbalance by improving model performance on hard-to-classify examples by focusing on wrongly classified examples. The technique for this is called down-weighting, and uses the prediction probabilities to dynamically adjust the contribution of each sample to the loss. Well-classified examples with high probabilities $p_y$ are down-weighted by adding a modulating factor $(1 - p_y)^\gamma$ to the cross-entropy loss (eq. \eqref{eq:ce_loss}). Here, $\gamma \geq 0$ is a tunable focusing parameter. The Focal Loss modifies the CE loss by applying the inverse prediction probability as follows:

\begin{equation}
    \label{eq:focal_loss}
    L_{fl} = -(1 - p_y)^\gamma \log(p_y)
\end{equation}

This factor increases the weight of misclassified examples, ensuring the model prioritizes learning from challenging samples. Figure \ref{fig:focal_loss} shows how the focal loss for different values of $\gamma$. When $\gamma = 0$ the modulating factor equals 1, and the focal loss becomes the standard CE loss. The blue line in figure \ref{fig:focal_loss} represents the standard cross-entropy loss for $\gamma = 0$.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.6\textwidth]{Images/focal_loss.png}
    \caption{Figure from \cite{lin2018focallossdenseobject}. \todo{Make this figure}}
    \label{fig:focal_loss}
\end{figure}

\paragraph{Class-Balanced Loss}
\emph{Class-Balanced (CB) Loss} \cite{cui2019classbalancedlossbasedeffective} introduces a re-weighting strategy based on the effective number of samples per class to re-balance the loss.

Instead of simply using raw class frequencies, CB Loss estimates how much additional information new samples provide, acknowledging an information overlap among data, and as the number of samples increases, the marginal benefit of extracted features from new data diminishes. As illustrated in figure \ref{fig:cb_featue_space}, the feature space is the map of all possible data, and each sample occupies a part of the space. Collecting more samples of a class means that there is a probability that their features overlap, meaning that additional samples diminishes new information.
In feature space, the probability of newly sampled data with volume 1 is overlapping $p$, which means that the probability of adding new information to the feature space is $(1-p)$. 


\begin{figure}[h!]
    \centering
    \includegraphics[width=0.6\textwidth]{Images/CB_feature_space.png}
    \caption{Figure from \cite{cui2019classbalancedlossbasedeffective}. \todo{Make this figure}}
    \label{fig:cb_featue_space}
\end{figure}

Cui et al. \cite{cui2019classbalancedlossbasedeffective} introduced the effective number of samples as the expected volume of samples, denoted $E_n$, where $n \in \mathbb{Z}_{>0}$ is the number of sampels. The effective number of samples 
is defined as:

\begin{equation}
    \label{eq:eff_num}
    E_n = \frac{1-\beta^n}{1-\beta}
\end{equation}

where $\beta = (N-1)/N$. The hyperparameter $\beta \in [0,1)$ determines how fast $E_n$ grows.

As a result, the CB loss introduces a class-balanced re-weighting term, inversely proportional to the effective number of classes. Adding the CB term to the standard softmax cross-entropy given in equation \eqref{eq:ce_loss} yield the following:

\begin{equation}
    \label{eq:cb_loss}
    L_{cb} = - \frac{1 - \beta}{1 - \beta^{n_y}} \log(p_y)
\end{equation}

where $n_y$ is the number of samples in the ground truth class $y$. Applying the inverse of the effective number ensures that minority classes contribute more to the total loss, as the effective number in equation \eqref{eq:eff_num} grows large for minority classes (small $n_y$) and small for majority classes (large $n_y$). When $\beta = 0$ the loss is equivalent to the CE loss. Contrary, when $\beta \longrightarrow 1$ the re-weighting effect grows.




\paragraph{Equalization Loss}
\emph{Equalization (EQ) Loss} \cite{tan2020equalizationlosslongtailedobject} aims to mitigate the over-suppression of tail classes, which occurs when these underrepresented classes serve predominantly as negative examples for the more frequent classes. The idea is to ignore the gradients for rare classes so that they are not excessively penalized when they appear as negatives, preventing their learned representations from being overshadowed. An analysis of gradient impact on classes is seen in figure \ref{fig:eql_gradients}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\textwidth]{Images/gradient_analysis.png} 
    \caption{Gradient analysis. \cite{tan2020equalizationlosslongtailedobject}.}
    \label{fig:eql_gradients} 
\end{figure}

The EQ loss was designed for object recognition, but the authors decided to adopt the principles into image classification, officially called the \emph{Softmax Equalization (SEQ) Loss}, defined as: 

\begin{equation}
    \label{eq:EQ_loss}
    \mathcal{L}_{SEQL} = - \sum_{j=1}^{C} y_j \log(\tilde{p}_j)
\end{equation}

where

\begin{equation}
    \tilde{p}_j = \frac{e^{z_j}}{\sum_{k=1}^{C} \tilde{w}_k e^{z_k}}
\end{equation}

and the weight term is given by:

\begin{equation}
    \tilde{w}_k = (1 - \beta T_\lambda(f_k))(1 - y_k)
\end{equation}

Here, $y_j$ is the ground truth class, $z_j$ is the logit for class $j$, $f_k$ is the frequency of class $k$, \(T_\lambda(\cdot)\) is the threshold on class frequency. As there is no background category for image classification tasks, $\beta$ is introduced to randomly maintain the gradient of negative samplers. \(\beta\) is a random variable with probability \(\gamma\) of taking value 1 and probability \(1 - \gamma\) of taking value 0.




\subsubsection{Logit Adjustment}
\label{sec:logit_adjust}
Logit adjustment is a class re-balancing technique that aims to optimize the class imbalance by adjusting the model outputs, i.e. prediction logits, typically based on prior class probabilities \cite{menon2021longtaillearninglogitadjustment,ren2020balancedmetasoftmaxlongtailedvisual}. \todo{Write about the label distribution shifts between training and test data.} 

The output model outputs are computed by the following equation:

\begin{equation}
    \label{eq:logits}
    \eta_j = \theta^T_j f(x)
\end{equation}

where $\theta_j$ are the weights for the last layer for class $j$, and $f(x)$ is the feature extractor function \cite{ren2020balancedmetasoftmaxlongtailedvisual}.

\paragraph{Balanced Softmax Loss}
The \emph{Balanced Softmax (BS)} Loss modifies the standard softmax loss (eq. \eqref{eq:ce_loss}) by directly incorporating class priors $\pi_y$ into the logits before computing probabilities \cite{ren2020balancedmetasoftmaxlongtailedvisual}. Unlike approaches that operate solely in the loss space, BS Loss integrates class frequency adjustments at the probability computation stage, effectively neutralizing the bias introduced by imbalanced class distributions. 

In \emph{Balanced Meta-Softmax for Long-Tailed Visual Recognition}, the authors represent Softmax regression as a multinomial distribution, $\phi$, dependent on $\eta$, defined as:

\begin{equation}
    \phi_j = \frac{e^{\eta_j}}{\sum_{i=1}^{k}e^{\eta_i}}
\end{equation}

where $k$ are the number of classes.

The Balanced Softmax uses the output logits $\eta$ (eq. \eqref{eq:logits}) to parametize two separate probabilities: $\phi$ for testing, and $\hat{\phi}$ for training. This separation is used when training with an imbalanced dataset to eliminate the discrepancy between training and testing distributions \cite{ren2020balancedmetasoftmaxlongtailedvisual}.

Assume $\phi$ to be the desired conditional probability of the balanced dataset, with the form $\phi_j = p(y = j \mid x) = \frac{p(x \mid y=j)}{p(x)} \cdot \frac{1}{k}$, and $\hat{\phi}$ to be the desired conditional probability of the imbalanced training set, with the form $\hat{\phi}_j = \hat{p}(y = j \mid x) = \frac{p(x \mid y=j)}{\hat{p}(x)} \cdot \frac{n_j}{\sum_{i=1}^{k} n_i}$. If $\phi$ is expressed by the standard Softmax function of the model output $\eta$, then $\hat{\phi}$ can be expressed as:

\begin{equation}
    \hat{\phi}_j = \frac{n_j e^{\eta_j}}{\sum_{i=1}^{k} n_i e^{\eta_i}}.
\end{equation}

where $n_j$ is the number of samples in class $j$.

The authors proved that this method can accomodate the label distribution shifts between training and test sets \cite{ren2020balancedmetasoftmaxlongtailedvisual}. ultimately, the Balanced Softmax loss function is becomes:

\begin{equation}
    \mathcal{L}_{bs} = - \log\left( \frac{\pi_y \exp(z_y)}{\sum_j \pi_j \exp(z_j)} \right)
\end{equation}

where $z_y$ represents the logit for the true class, $\pi_y$ is the class prior (e.g. normalized frequency of samples from class $y$), and the term in the denominator normalizes the probabilities across all classes while accounting for priors.


\subsubsection{Margin Modification (Re-Margining)}
\label{sec:margin_mod}
Re-margining aims to solve class imbalance by improving classification by introducing margin between classes, encouraging higher margins between rare classes. 

The margin $\gamma_i$ of the $i$-th class is the minimum distance of the data in the $i$-th class to the decision boundary. Figure \ref{fig:decision_boundaries} illustrates how the margin affects the decision boundary for binary classification. As the decision boundary separates the regions of input space corresponding to different class predictions, by increasing the margin, the model becomes more confident in classification.


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{Images/decision_boundary.png} 
    \caption{Margins \cite{cao2019learningimbalanceddatasetslabeldistributionaware}. \todo{make this figure.}}
    \label{fig:decision_boundaries} 
\end{figure}

The margin is defined as:

\begin{equation}
    \label{eq:margin_def}
    \gamma(x,y) = f(x)_y - \max_{j \neq y}f(x)_j
\end{equation}

which is the distance between the prediction of the sample $(x,y)$ and the decision boundary.

The training margin for class $j$ is defined as:

\begin{equation}
    \label{eq:margin_class_def}
    \gamma_j = f(x)_y - \max_{i \in S_j}\gamma(x_i,y_i)
\end{equation}

The classical generalization bound is defined as

\begin{equation}
    \label{eq:gen_bound}
    \text{imbalanced test error}\lesssim \frac{1}{\gamma_{min}}\sqrt{\frac{C(\mathcal{F})}{n}}
\end{equation}

In a balanced setting:

\begin{equation}
    \frac{1}{2}\text{err}[\text{class1}]+\frac{1}{2}\text{err}[\text{class2}] \lesssim \left(\frac{1}{\gamma_1\sqrt{n_1}}+\frac{1}{\gamma_2\sqrt{n_2}}\right)\sqrt{C(\mathcal{F})}
\end{equation}

This boundary can be shifted by taking the minimum: $\min{\gamma_1\sqrt{n_1}}+\frac{1}{\gamma_2\sqrt{n_2}}$, such that $\gamma_1+\gamma_2=C$. This yields the optimal solution:

\begin{equation}
    \label{eq:opt_margin}
    \gamma_i\varpropto n^{-1/4}_i 
\end{equation}

with the standard solution being $\gamma_1=\gamma_2$ \cite{cao2019learningimbalanceddatasetslabeldistributionaware}.

\paragraph{LDAM Loss}
The \emph{Label-Distribution-Aware Margin (LDAM)} Loss \cite{cao2019learningimbalanceddatasetslabeldistributionaware} represents a class-sensitive approach based on re-margining. Instead of altering the loss directly, LDAM modifies the margin applied to each class’ decision boundary, ensuring that the model is more confident in classification for minority classes. The goal is to apply the optimal boundary in equation \eqref{eq:opt_margin} by increasing the distance between the largest and secon largest logit for each class and introducing that to the cross-entropy loss (eq. \eqref{eq:ce_loss}).


The class-dependent margin for multiple classes is defined as:

\begin{equation}
    \label{eq:margin}
    \gamma_j = \frac{C}{n^{1/4}_j}
\end{equation}

where $C$ is a tunable hyper-parameter, and $n_j$ is the number of samples in class $j$. Applying the margin through the loss function, inspired by the Hinge Loss \todo{reference}, will encourage the model to respect the margins in equation \eqref{eq:margin}:

\begin{equation}
    \label{eq:ldam-hg}
    \mathcal{L}_{ldam-hg} = \max\left(\max_{j \neq y}\{z_j\} - z_y + \Delta_y, 0\right)
\end{equation}

where

\begin{equation}
    \Delta_j = \frac{C}{n_j^{1/4}} \quad \text{for } j \in \{1, \dots, k\}.
\end{equation}

and $z_y$ is the model output (logit) for the true class $y$, and $\max_{j \neq y}\{z_j\}$ is the highest score among incorrect classes. Adding the margin $\Delta_j$ ensures that the true class logit is separated from the highest incorrect score by the at least $\Delta_j$. However, since the function in equation \eqref{eq:ldam-hg} is non-differentiable at certain points, a modifications to the cross-entropy loss is used to address this. Here, the class-dependent margins are introduced directly into the logits as follows:

\begin{equation}
    \label{eq:ldam}
    L_{ldam} = - \log\left( \frac{\exp(z_y - \Delta_y)}{\sum_j \exp(z_j - \Delta_j)} \right)
\end{equation}