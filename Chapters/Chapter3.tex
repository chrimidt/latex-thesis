% Chapter 3: Methodology

This chapter describes the methods and approaches used in the experiments. This includes dataset preparation, models, loss functions, etc.

\section{Overview of Approach}
An overall description of the approach to tackling the long-tailed dataset problem, including an explanation of the strategy, 
such as balancing techniques and model selection.

\section{Dataset Preparation and Specifications}
A description of this section here.

Following the dataset structure used in \textit{Deep Long-Tailed Learning: A Survey}, the CIFAR100 dataset was modified to create a long-tailed training set and a balanced test set. Key details include:

\begin{itemize}
    \item Dataset characteristics: Number of classes, imbalance ratio, and train-test splits.
    \item Preprocessing steps: Resizing, normalization, and augmentation techniques.
    \item Handling imbalance: Techniques like re-sampling and augmentation to address the long-tailed distribution.
\end{itemize}

\subsection{Data Characteristics: Class Distribution}
\textit{A list of subjects to include in this section:}

\begin{itemize}
    \item Describe the ImageNet-LT dataset: number of classes, imbalance ratio, etc.
    \item Describe the plots and what they mean for the CIFAR100-LT data preparation. 
\end{itemize}

The first step is to prepare the data for training and testing. In order to generate training, validation, and test datasets that resemble the datasets used for the empirical studies in \textit{Deep Long-Tailed Learning: A Survey} \cite{zhang2023deep}, their dataset are investigated: The GitHub repository \cite{VanintLT} for the paper \textit{Deep Long-Tailed Learning: A Survey} was downloaded and an environment was set up on the Jupyter Hub on the Freja node on the ece cluster. The \texttt{.txt} files with the data from ImageNet-LT (\texttt{ImageNet\_LT\_train.txt}, \texttt{ImageNet\_LT\_val.txt}, \texttt{ImageNet\_LT\_test.txt}) are shown on figures \ref{fig:IN-train}, \ref{fig:IN-val}, and \ref{fig:IN-test}, respectively. 

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{Images/Plots/class_distribution_train.png}
    \caption{The class distribution of the training images for the ImageNet-LT dataset shows a long-tailed distribution.}
    \label{fig:IN-train}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{Images/Plots/class_distribution_val.png}
    \caption{The class distribution of the validation images for the ImageNet-LT dataset shows that there are 20 samples of each class.}
    \label{fig:IN-val}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{Images/Plots/class_distribution_test.png}
    \caption{The class distribution of the test images for the ImageNet-LT dataset shows that there are 50 samples of each class.}
    \label{fig:IN-test}
\end{figure}

\subsection{Preparation: CIFAR100-LT}
\textit{A list of subjects to include in this section:}

\begin{itemize}
    \item Briefly describe the CIFAR100 dataset, and why it was chosen as the primary dataset for this thesis. Refer to section 2.1.
    \item Insert plots of the CIFAR100 dataset.
    \item Describe the generation of the imbalanced dataset: IMBALANCECIFAR100 method from the LDAM-DRW paper.
    \item Describe the imbalance ratio.
    \item Explain why the dataset was saved, and not generated in run-time, like in LDAM-DRW.
    \item Explain why the dataset was split into 450 samlpes per class for training and 50 samples per class for testing.
    \item Insert plots of the imbalanced dataset.
    \item Describe the head, middle, and tail classes.
    \item Insert plot of the division of the long-tailed test dataset: head, middle, and tail classes.
    \item Explain the purpose of the division.
    \item Potentially a few images from the CIFAR100 dataset.
    \item Argument as to why the training data was split into training and test.
\end{itemize}

The experiments conducted in this thesis primarily utilize the CIFAR-100 dataset.

The dataset was downloaded using the PyTorch torchvision.datasets.CIFAR100 utility. The training and test datasets were preprocessed by converting the images to tensors using the ToTensor transformation and saved as .pth files for efficient loading during experiments. 

To address the needs of the experiments in this thesis, the original CIFAR-100 dataset was modified to create a new split of the training data. Specifically, the training data was split into 450 samples per class for training and 50 samples per class for testing, maintaining the class distribution within these splits. The original test set of the CIFAR-100 dataset was retained as the validation set for training and evaluation purposes.

\textbf{Training Set:} To simulate real-world scenarios with class imbalances, the training dataset was modified to introduce an exponential imbalance across the 100 classes. The imbalance was created using the quantile Pareto distribution in equation \eqref{eq:pareto}, where the number of samples per class decreases exponentially, controlled by the imbalance factor. For this thesis, an imbalance factor of 0.01 was applied. This means that the most frequent class contains significantly more samples than the least frequent class. 

The resulting class distribution varied from the most frequent class having 450 samples to the least frequent class having only \todo{check} samples.  This imbalance ensured no class was left with zero samples, maintaining the integrity of all classes for training. \todo{see figure \ref{fig:cifar100_imbalance_cifar}.}

\textbf{Test Set:} To evaluate the performance of the model under similar conditions to the imbalanced training set, an imbalanced test set was created from the previously split test dataset. The imbalance in the test set mirrors the exponential distribution used for the training data, with the same imbalance factor of 0.01. The class distribution in the test set follows the same order of classes (from most to least frequent) as the imbalanced training set. No class has fewer than one sample.

Describe the class distribution in figures \ref{fig:cifar100_imbalance_cifar} and \ref{fig:cifar100val}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{Images/Plots/Class Distribution for CIFAR100-LT.png}
    \caption{The class distribution of CIFAR100-LT with imbalance ratio 100 generated by the imbalance\_cifar.py in the LDAM-DRW GitHub repository \cite{kaidic_ldam_drw}.}
    \label{fig:cifar100_imbalance_cifar}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{Images/Plots/cifar100_val_class_distribution.png}
    \caption{The class distribution of the CIFAR100 validation set from torchvision [reference here].}
    \label{fig:cifar100val}
\end{figure}

\todo{Include plots of head, middle and tail test splits.}

\subsection{Data Augmentation}
Not sure if this should be a section, or if it should be in the experimental setup section.

Describe what data augmentation was used on the training data.

\section{Long-tailed Learning Techniques}
Description of the specific methods used to address class imbalance, such as data sampling, class re-weighting, etc. 
Justification for selecting these techniques, potentially referencing prior research (from Deep Long-Tailed Learning: A Survey by Zhang et al.).

\subsection{Model Selection}
\textit{A list of subjects to include in this section:}

\begin{itemize}
    \item Mention the model architectures chosen for training, and describe why they are appropriate for deep long-tailed learning with reference to the background section.
    \item Discuss the strengths and limitations of these models in addressing the challenges posed by imbalanced data.
    \item Describe how they were pretrained (ImageNet-1K, ImageNet-21K) and what that means for the training on CIFAR100.
\end{itemize}


\subsection{Selection of Loss Function}
\textit{A list of subjects to include in this section:}

 \begin{itemize}
    \item Describe the different loss functions and why they are appropriate for deep long-tailed learning with reference to the background section.
    \item Rationale for each loss function's inclusion, focusing on its expected benefits for imbalanced classes and how it adresses the bias toward majority classes.
    \item Write pseudo code of the implementations
 \end{itemize}

 \paragraph{Softmax Loss}
 Without any additional weighting or margin adjustments, the standard CE loss tends to give well-represented classes an advantage. Since head classes have more training examples, each of their samples also serves as a “negative” example for all other classes, providing them with a disproportionately high number of beneficial gradients. In contrast, tail classes, having fewer positive samples, are both less represented positively and more often negatively suppressed \cite{zhang2023deep, lin2018focallossdenseobject}. As a result, the model learns biased decision boundaries, performing well on frequent classes but poorly on rare ones. The CE loss thus serves as a natural starting point or baseline from which various class-sensitive modifications are derived. These modifications directly address the imbalance issue by altering the training dynamics, ensuring a more equitable distribution of gradients and improving the final model’s performance on underrepresented classes.

 \paragraph{Weighted Softmax Loss}
 (1) In scenarios of severe imbalance, minority classes are often overshadowed by the abundant head classes. WCE addresses this by scaling the loss for each class according to a class-dependent factor, typically the inverse of its frequency, thereby placing a heavier penalty on misclassification of rare classes \cite{zhang2023deep}.
(2) By elevating the importance of tail classes in the loss, the model is incentivized to allocate more representational capacity to them, improving recall and reducing class-specific error rates.
(3) Although simple, WCE provides a direct and intuitive method for re-weighting: each class’s contribution to the parameter updates is modulated to reflect its scarcity or importance in the dataset distribution.
(4) This helps maintain a more balanced gradient flow during training, preventing head classes from monopolizing the optimization trajectory.
(5) However, one limitation is that the choice of class weights is often empirical. Naively using the inverse class frequency may not always yield optimal results, and more sophisticated weighting heuristics or data-driven methods can be employed \cite{zhang2023deep}.
(6) WCE sets a straightforward precedent for other re-weighting schemes, serving as a baseline for more elaborate methods that try to adapt the weights dynamically during training.
(7) As it is easy to implement and understand, WCE is frequently considered as an initial step towards handling imbalance before exploring more complex techniques.
(8) Overall, WCE exemplifies how re-weighting can effectively guide the model to learn less-represented categories more robustly, thereby improving long-tailed recognition performance.

\paragraph{Focal Loss}
(1) Focal Loss modifies the CE loss by including a focusing parameter that down-weights well-classified examples, thus reducing the dominance of easily recognized classes or samples \cite{lin2018focallossdenseobject}.
(2) By concentrating on hard examples—often those from underrepresented classes—Focal Loss ensures that the model’s parameter updates focus on improving performance where it struggles the most.
(3) This dynamic weighting effectively reduces the overwhelming influence of head classes, since these classes tend to produce more confidently correct predictions that are down-weighted.
(4) Consequently, tail classes, which inherently present more challenging classification problems, receive increased attention and a larger share of meaningful updates.
(5) The intuition is that by penalizing confident yet trivial predictions less, the model has “budget” to allocate its learning capacity towards harder or rarer samples, thereby mitigating the imbalance effect.
(6) Empirical results have shown that this approach can significantly boost performance on minority classes without severely degrading performance on majority classes \cite{zhang2023deep}.
(7) Since Focal Loss does not depend solely on class frequency, it can adapt to the evolving difficulty distribution of samples during training, making it more flexible than static frequency-based weights.
(8) Though originally introduced in the context of object detection, it has been widely adopted in image classification tasks with long-tailed distributions.
(9) Thus, Focal Loss stands as a prime example of a re-weighting strategy that leverages model feedback (prediction confidence) rather than static priors (class frequencies) to balance training.

\paragraph{Class-Balanced Loss}
(1) The key insight is that each class’s influence should not be linearly scaled by its frequency alone. Instead, it considers that as class size grows, its incremental value decreases, suggesting that fewer samples are needed from frequently seen classes to establish robust representations.
(2) By formulating a class-specific weight derived from the effective sample number, CB Loss prevents large classes from dominating the training gradient simply due to their sheer volume of examples.
(3) This leads to a more stable and theoretically grounded re-weighting mechanism than naive inverses of class frequency, and can yield better generalization on minority classes.
(4) The concept of effective number is computed using a parameter $\gamma$ to determine how quickly the marginal benefit of additional samples decreases.
(5) This approach balances the training signal by giving classes a weight proportional to the inverse of their effective number, essentially normalizing the influence of different classes to a more comparable scale \cite{zhang2023deep}.
(6) As a result, CB Loss ensures that each class, regardless of its raw frequency, contributes meaningful gradients that reflect both its representation level and its effective complexity.
(7) This method elegantly merges the ideas of frequency-based weighting with a diminishing return model, acknowledging that overly abundant classes do not linearly improve overall performance.
(8) With this refined weighting scheme, CB Loss tends to improve recognition of rare categories without unduly harming head class accuracy.
(9) In effect, CB Loss provides a solid theoretical foundation for re-weighting by linking data volume, information content, and balanced training signals.

\paragraph{Balanced Softmax Loss}
(1) Standard softmax-based training assumes a balanced class distribution during both training and testing, which is often unrealistic in real-world datasets.
(2) BS Loss recognizes that applying a correction to the logits rather than just weighting the final loss terms can more directly counter the skewed gradient flow that results from class imbalance \cite{zhang2023deep}.
(3) By multiplying logits by the class frequencies, it effectively adjusts the decision boundaries, ensuring that classes with fewer samples are not overshadowed by the dense representation of majority classes.
(4) This adjustment encourages the model to produce a more uniform posterior distribution that accurately reflects true class priors, both during training and inference.
(5) Consequently, the network learns a more balanced internal representation, fostering improved recognition of tail classes without overly penalizing head classes.
(6) Balanced Softmax offers a theoretically motivated way to incorporate known label distributions into the training process, aiming to produce unbiased probability estimates.
(7) It can be seen as a more direct approach to re-weighting at the logit level, contrasting with methods that focus on re-weighting the loss function after probability computation.
(8) Empirical results show that BS Loss can be more effective than naive weighting schemes, as it corrects the bias at its source rather than compensating after the fact.
(9) This method paves the way for other logit-adjustment techniques and underscores the importance of integrating class priors into the model’s probability estimation process.

\paragraph{Equalization Loss}
(1) In highly imbalanced scenarios, each positive sample from a head class can produce negative gradients for all other classes, including tail classes. Over time, this can discourage the network from correctly identifying these minority classes.
(2) EQ Loss counteracts this by selectively down-weighting the negative gradients that arise when tail classes appear as negative samples for majority classes \cite{zhang2023deep}.
(3) By reducing the negative influence on tail classes, EQ Loss ensures that the model does not develop an overly “head-biased” representation that fails to recognize minority categories.
(4) This strategy highlights a subtlety in the re-weighting paradigm: it is not only about boosting tail class importance, but also about preventing undue suppression of these classes through negative gradients.
(5) However, excessively down-weighting negative gradients may impair the model’s discriminative power. To address this, adaptive variants, such as ACSL and Equalization v2, adjust the amount of gradient suppression based on the model’s prediction confidence \cite{tan2020equalizationlosslongtailedobject, zhang2023deep}.
(6) This adaptive mechanism allows the network to selectively protect tail classes when needed, while still maintaining a strong discriminative boundary between classes.
(7) The result is a more balanced training signal that prevents rare classes from being “washed out,” thereby improving long-tailed recognition.
(8) EQ Loss exemplifies a more fine-grained approach to re-weighting, intervening at the gradient level to ensure fair treatment of minority categories.
(9) By focusing on the gradient flow rather than just the static weighting of loss values, EQ Loss and its successors illustrate the evolving sophistication of class-sensitive re-weighting techniques.

\paragraph{LDAM Loss}
(1) Margin-based losses generally improve generalization by enforcing a minimum separation between class decision boundaries.
(2) LDAM incorporates class-dependent margins that are inversely related to class frequencies, providing larger margins to tail classes and smaller margins to head classes \cite{zhang2023deep}.
(3) By increasing the margin for minority classes, it effectively lowers their decision boundary threshold, making it easier for the model to confidently classify these classes and reducing their misclassification rates.
(4) This approach contrasts with re-weighting methods: while re-weighting modifies the magnitude of the loss contributions, re-margining directly influences how decision boundaries are formed in the embedding space.
(5) The motivation behind LDAM is that the imbalance problem is not solely a matter of loss amplitude, but also how easily minority classes can carve out their own regions in the feature space.
(6) With larger margins for tail classes, LDAM encourages more robust and discrimination-friendly representations for these classes, thereby counteracting the bias induced by uneven sample distributions.
(7) As a result, the final classifier tends to perform better on rare categories, as it learns to keep them well-separated from other classes, even when their sample counts are low.
(8) Combining LDAM with re-weighting or re-sampling techniques can yield even stronger improvements, as each method addresses a complementary aspect of the imbalance issue.
(9) LDAM underscores that balancing the training process can be achieved not only by adjusting loss weights, but also by reshaping the geometry of the decision boundaries in class-sensitive ways.

\subsection{Excluded Long-Tailed Learning Methods}
This section will explain why some deep long-tailed learning techniques, like re-sampling, was not the focus of this thesis.

\section{Evaluation Metrics}
\textit{A list of subjects to include in this section:}

\begin{itemize}
    \item Describe common evaluation metrics used for classification tasks.
    \item Explain the choice of top-1 accuracy.
    \item Explain how the performance is assessed across different class groups.
    \item Explain the choice of F1-score.
\end{itemize}



\subsection{Model Comparison}
Metrics for comparison of overall model performance. Example: harmonic mean, geometric mean, min score. Mention why these metrics are suitable for balancing performance across head, middle, and tail classes.

\section{Reproducibility}
\textit{A list of subjects to include in this section:}

\begin{itemize}
    \item Use of random seed initialization.
    \item Documentation of dataset versions and codebase.
    \item Availability of scripts for dataset preparation and model training.
\end{itemize}

\section{Implementation Details}
Technical explanations of any unique or customized methods implemented in code, for example the custom dataset.

\textit{A list of subjects to include in this section:}

\begin{itemize}
    \item Explain how the models where implemented.
    \item Rationale for implementing the loss functions manually instead of copy existing repositories.
    \item Describe the benefits of copying existing repositories.
\end{itemize}

