% Chapter 4: Experimental Setup

This chapter focuses on the on the implementation details of the experiments conducted in this thesis. Here, the specifics of the training configurations are described. 

\paragraph{Dataset Specifications}
The experiments in this thesis are conducted with CIFAR-100 \cite{krizhevsky2009learning} benchmark. The CIFAR-100 is split into a training set of 45,000 samples and a test set of 5,000 samples. Furthermore, the CIFAR-100-LT was created from the training set with an imbalance factor of 100 with the number of samples per class ranging from 450 in the most frequent class to just 4 in the least frequent class. The test set mirrors this distribution and maintains the same imbalance factor. See section \ref{sec:dataset_specs}.

To further analyze model performance across different class frequencies, the imbalanced test set is divided into three subsets: the head test set, containing the top one-third most frequent classes, the middle test set, which includes the middle one-third of classes, and the tail test set. The validation set is the original CIFAR-100 test set with 10,000 samples distributed equally across the 100 classes.

\paragraph{Training Configuration}
All experiments are implemented using PyTorch \cite{paszke2019pytorchimperativestylehighperformance}, with models ResNet50 \cite{he2015deepresiduallearningimage}, MobileNetV2 \cite{sandler2018mobilenetv2}, and ConvNeXt Base \cite{todi2023convnext} initialized using pretrained weights on the ImageNet-1K dataset, and model ViT-B/16 \cite{dosovitskiy2021imageworth16x16words} with ImageNet-21K. Each model is modified to adapt to the CIFAR-100 dataset by replacing the final classification layer with a 100-class fully connected layer.

Training is performed over 90 epochs with a batch size of 128. The Adam optimizer \cite{kingma2017adammethodstochasticoptimization} is employed with an initial learning rate of 0.001, which is reduced by a factor of 0.1 every 30 epochs using a StepLR scheduler. The training is conducted on four NVIDIA TITAN X (Pascal, 12 GB). See appendix \ref{app:B} for further specifications.

\paragraph{Evaluation}
The metric used for performance evaluation is the top-1 accuracy.


\paragraph{Reproducibility}
To ensure reproducibility, a fixed random seed of 42 is used across all experiments, and GPU computations enforced to be deterministic. Configuration files in YAML format are used to document hyperparameters, dataset settings, and model configurations, allowing for exact replication of experiments. Datasets and model checkpoints are saved to facilitate reuse and further analysis.

