% Chapter 5: Results and Analysis
This chapter presents a detailed analysis of the experimental results, comparing model performances across head, middle, and tail classes, and evaluating the impact of different loss functions.

\section{Overview}
Present the performance of all tested models and methods.
Use tables or plots to summarize key results.
Highlight trends or notable observations across the methods.

\section{Main Findings}
% 1. Complete "Main Findings" Section
% This is the most critical part to finalize, as it sets the tone for the rest of the results chapter.
% Go through your tables and add specific examples of findings for each claim. For example:
% "Balanced Softmax consistently improved tail-class accuracy across all models, with a notable improvement in MobileNetV2 (Acc1: 0.9211)."
% "LDAM performed comparably on middle classes but struggled on head classes, as observed in ResNet50V2 (Acc1: 0.7983)."
% This will eliminate the TODO placeholders and give the reader a clear summary of the results.

Describe main findings.

Example:

"Model X achieved an accuracy of Y \% on the balanced test set."
"F1 score for the imbalanced test set showed an improvement using Loss A."
"The MobileNetV2 model, trained with Focal Loss, achieved a higher F1 score compared to models trained with Cross-Entropy Loss."

This section summarizes the overall performance of all tested models using tables and plots to highlight key trends and notable observations.

Across all models, loss functions like TODO and TODO showed improved performance on tail classes, while TODO often underperformed in imbalanced scenarios.

\section{Comparison of Models}
Analyze how different models impact performance.
Use visualizations to compare results.
Discuss strengths and weaknesses of each model.

\subsection{MobileNetV2}
Table \ref{tab:mobilenet_bal_acc1_1} shows the top 1 accuracies for MobileNetV2 on various loss functions.

% \begin{itemize}
%     \item Best performance on balanced test data.
%     \item Best performance on long tailed test data.
%     \item Best performance on head classes.
%     \item Best performance on tail classes.
%     \item Worst performance on blananced test data.
%     \item Worst performance on long tailed test data.
%     \item Worst performance on head classes.
%     \item Wrost performance on tail classes.
%     \item Comment on the why the results on the middle classes are worse than both head and tail classes.
%     \item Comment on curious results on the middle classes for softmax, focal loss, weighted softmax, and class-balanced loss.
%     \item Comment on the curious results on the tail classes for balanced softmax loss, equalization loss and LDAM loss.
% \end{itemize}

\begin{table}[H]
    \centering
    \begin{tabular}{cccccc}
        \toprule
        Loss Function & Balanced & Long-tailed & Head & Middle & Tail \\ 
        \midrule
        Softmax   & 0.7978   & 0.8059 & 0.8069 & 0.7870 & 0.8684 \\
        Focal loss   & 0.8014   & 0.8011 & 0.7998 & 0.7870 & 0.8947 \\
        Weighted Softmax loss   & 0.7978   & 0.8059 & 0.8069 & 0.7870 & 0.8684 \\
        Class-balanced loss   & 0.7978   & 0.8059 & 0.8069 & 0.7870 & 0.8684 \\
        Balanced Softmax loss   & 0.8034  & 0.8030 & 0.8069 & 0.7574 & 0.9211 \\
        Equalization loss   & 0.7994   & 0.8040 & 0.8057 & 0.7692 & 0.9211 \\
        LDAM loss   &  0.7828   & 0.7821 & 0.7808 & 0.7574 & 0.9211 \\
        \bottomrule
    \end{tabular}
    \caption{Top-1 accuracy results for MobileNetV2 on the balanced dataset across all loss functions.}
    \label{tab:mobilenet_bal_acc1_1}
\end{table}

From Table \ref{tab:mobilenet_bal_acc1_1}, the overall best performance on MobileNetV2 trained with a balanced CIFAR100 training dataset is achieved by Balanced Softmax Loss, which has the highest accuracy on the balanced test dataset (Acc1: 0.8034), as well as on the head (Acc1: 0.8069) and tail (Acc1: 0.9211) classes, with only slightly worse performance on the middle classes in comparison. Among all loss functions, LDAM Loss shows the lowest overall performance on the balanced test set (Acc1: 0.7828) and the long-tailed test set (Acc1: 0.7821), except for its strong performance on tail classes (Acc1: 0.9211). % This highlights LDAM’s specialized focus on tail classes at the cost of performance on head and middle classes.

Softmax Loss, Weighted Softmax Loss, and Class-Balanced Loss yield the same accuracies across all test datasets, likely due to their similarities in loss design [TODO: Refer to background section].

Balanced Softmax Loss, Equalization Loss, and LDAM Loss exhibit the highest accuracy on tail classes (Acc1: 0.9211). Despite their differing loss designs, this convergence in accuracy suggests that the dataset's tail-class performance may have reached a plateau, possibly due to the inherent characteristics of the tail classes, i.e. either noise or the limited number of samples available per class in the tail [TODO: Refer to background section].

A similar trend is observed in the middle-class accuracy, where Softmax, Focal Loss, Weighted Softmax Loss, and Class-Balanced Loss all achieve identical results (Acc1: 0.7870). Similarly, for head classes, Softmax, Weighted Softmax Loss, Class-Balanced Loss, and Balanced Softmax Loss perform equally well, achieving the highest accuracy (Acc1: 0.8069), hinting at saturation.  % This consistency across different loss functions on the balanced dataset could be due to their shared design principles.





% However, because balanced softmax loss does not yield the same result on both the balanced test set and the long-tailed test set as the other three mentioned, it is likely due to a saturation in the head classes. The softmax loss, weighted softmax loss, and class-balanced loss all yield the same performance across test sets, showing their similiarity in loss design on a balanced training dataset. The best perfoming loss function on the balanced test set is the weighted softmax loss (Acc1: 0.8034), however this is not the best performing loss function on the complete long-tailed dataset, but it has the best accuracy on tail classes (Acc1: 0.9211), and comparable performance on head and tail classes. Except for the performance on tail classes, the overall worst performing loss method is the LDAM loss, with accuracy of 0.7828 on a balanced test set, and 0.7821 on a long-tailed test set. Compared to the baseline, the softmax loss, that has an accuracy of 0.7978 on the balanced test set, and 0.8059 on the long-tailed test set.


% Balanced Softmax Loss, however, stands out due to its differentiated performance across datasets. Unlike Softmax, Weighted Softmax Loss, and Class-Balanced Loss, which yield consistent results across balanced and long-tailed datasets, Balanced Softmax achieves a higher accuracy on tail classes while showing slight variability in overall performance. This suggests that Balanced Softmax is more sensitive to class-specific adjustments, particularly in imbalanced scenarios.


% 
% Compared to the baseline Softmax Loss (balanced: Acc1: 0.7978, long-tailed: Acc1: 0.8059), Balanced Softmax Loss demonstrates slightly better performance on the balanced dataset but shows more variability on the long-tailed dataset, where it achieves comparable results on tail classes but lower performance overall.


Table \ref{tab:mobilenet_lt_acc1_1} show the top 1 accuracies for MobileNetV2 on various loss functions.

\begin{itemize}
    \item Best performance on balanced test data.
    \item Best performance on long tailed test data.
    \item Best performance on head classes.
    \item Best performance on tail classes.
    \item Worst performance on blananced test data.
    \item Worst performance on long tailed test data.
    \item Worst performance on head classes.
    \item Wrost performance on tail classes.
    \item Comment on the curious results on the middle classes.
\end{itemize}

\begin{table}[H]
    \centering
    \begin{tabular}{cccccc}
        \toprule
        Loss Function & Balanced & Long-tailed & Head & Middle & Tail \\ 
        \midrule
        Softmax   & 0.5282   & 0.7735 & 0.8341 & 0.5917 & 0.2368 \\
        Focal loss   & 0.5200   & 0.7745 & 0.8389 & 0.5917 & 0.1579 \\
        Weighted Softmax loss   & 0.5016   & 0.7231 & 0.7808 & 0.5503 & 0.2105 \\
        Class-balanced loss   & 0.1936   & 0.0913 & 0.0521 & 0.2485 & 0.2632 \\
        Balanced Softmax loss   & 0.5796   & 0.7650 & 0.8069 & 0.6331 & 0.4211 \\
        Equalization loss   & 0.5310   & 0.7650 & 0.8235 & 0.5917 & 0.2368 \\
        LDAM loss   & 0.4264 & 0.5899 & 0.6137 & 0.5444 & 0.2632 \\
        \bottomrule
    \end{tabular}
    \caption{Evaluation results for MobileNetV2 trained on the long-tailed dataset showing Acc1.}
    \label{tab:mobilenet_lt_acc1_1}
\end{table}

\subsubsection{Comparison}

The comparison of my experiment to the original MobileNetV2 trained on the original CIFAR100 dataset.

Table \ref{tab:comparison} compares the experimental setup and results of MobileNetV2 trained on the CIFAR100 dataset with those reported in Study A3. Key differences include the loss function, optimizer, and data augmentation techniques used.

While our approach achieves competitive top-1 accuracy on balanced datasets, the performance gap on long-tailed datasets highlights potential areas for improvement in augmentation and learning rate strategies.

TODO: Move comparisons of results to a seperate section. Keep only the results, put specification in an appendix.

\begin{table}[H]
    \centering
    \begin{tabular}{lp{5cm}p{5cm}}
        \toprule
        \textbf{Aspect} & \textbf{Your Experiment} & \textbf{Their Study (A3)} \\ 
        \midrule
        Dataset            & CIFAR100 Customized                & CIFAR100                \\
        Loss Function      & Softmax Cross-Entropy   & Binary Cross-Entropy    \\
        Epochs             & 90                      & 100                     \\
        Optimizer          & Adam                    & LAMB                    \\
        Learning Rate      & Step decay at 30, 60 epochs & Cosine decay from 0.005 or 0.008 \\
        Augmentation       & Resize (224x224), RandomCrop, Horizontal Flip, Normalize & RandAugment, Mixup, CutMix, Normalize \\
        Hardware           & 4x NVIDIA TITAN X (Pascal, 12GB) & 4x NVIDIA V100 (32GB) \\
        Evaluation Metric  & Top-1 Accuracy          & Top-1 Accuracy          \\
        Top-1 Accuracy     & 79.8 \% & 86.2\%-86.9\%           \\
        \bottomrule
    \end{tabular}
    \caption{Comparison of MobileNetV2 on CIFAR100 with their study (Procedure A3) \cite{wightman2021resnetstrikesbackimproved}.}
    \label{tab:comparison}
\end{table}


\subsection{ResNet50V2}

Table \ref{tab:resnet_bal_acc1_1} show the top 1 accuracies for ResNet50V2 on various loss functions.

\begin{itemize}
    \item Best performance on balanced test data.
    \item Best performance on long tailed test data.
    \item Best performance on head classes.
    \item Best performance on tail classes.
    \item Worst performance on blananced test data.
    \item Worst performance on long tailed test data.
    \item Worst performance on head classes.
    \item Wrost performance on tail classes.
    \item Comment on the curious results on the middle classes.
\end{itemize}

\begin{table}[H]
    \centering
    \begin{tabular}{cccccc}
        \toprule
        Loss Function & Balanced & Long-tailed & Head & Middle & Tail \\ 
        \midrule
        Softmax loss   & 0.8324  & 0.8421 & 0.8448 & 0.8047 & 0.9474 \\
        Focal loss   & 0.8310  & 0.8344 & 0.8341 & 0.8166 & 0.9211 \\
        Weighted Softmax loss   & 0.8324 & 0.8421 & 0.8448 & 0.8047 & 0.9474 \\
        Class-balanced loss   &  0.8324 & 0.8421 & 0.8448 & 0.8047 & 0.9474 \\
        Balanced Softmax loss   & 0.8310 & 0.8430 & 0.8460 & 0.8107 & 0.9211 \\
        Equalization loss   & 0.8292 & 0.8373 & 0.8412 & 0.7929 & 0.9474 \\
        LDAM loss   & 0.7990 & 0.7983 & 0.8069 & 0.7337 & 0.8947 \\
        \bottomrule
    \end{tabular}
    \caption{Evaluation results for ResNet50V2 trained on the custom balanced dataset, showing Acc1.}
    \label{tab:resnet_bal_acc1_1}
\end{table}

Table \ref{tab:resnet_lt_acc1_1} show the top 1 accuracies for ResNet50V2 on various loss functions.

\begin{itemize}
    \item Best performance on balanced test data.
    \item Best performance on long tailed test data.
    \item Best performance on head classes.
    \item Best performance on tail classes.
    \item Worst performance on blananced test data.
    \item Worst performance on long tailed test data.
    \item Worst performance on head classes.
    \item Wrost performance on tail classes.
    \item Comment on the curious results on the middle classes.
\end{itemize}

\begin{table}[H]
    \centering
    \begin{tabular}{cccccc}
        \toprule
        Loss Function & Balanced & Long-tailed & Head & Middle & Tail \\ 
        \midrule
        Softmax loss   & 0.5522 & 0.7954 & 0.8531 & 0.6391 & 0.2105 \\
        Focal loss   & 0.5456 & 0.7935 & 0.8483 & 0.6272 & 0.3158 \\
        Weighted Softmax loss   & 0.4976 & 0.7336 & 0.7915 & 0.5562 & 0.2368 \\
        Class-balanced loss   & 0.2052 & 0.1836 &  0.1445 & 0.3787 & 0.1842 \\
        Balanced Softmax loss   & 0.5908 & 0.7916 & 0.8270 & 0.6568 & 0.6053 \\
        Equalization loss   & 0.5452 & 0.7897 & 0.8389 & 0.6450 & 0.3421 \\
        LDAM loss   & 0.3742 & 0.5937 & 0.6469 & 0.4438 & 0.0789 \\
        \bottomrule
    \end{tabular}
    \caption{Evaluation results for ResNet50V2 trained on the long-tailed dataset, showing Acc1.}
    \label{tab:resnet_lt_acc1_1}
\end{table}



\subsection{ViT-B/16}

Table \ref{tab:vit_bal_acc1_1} show the top 1 accuracies for ViT-B/16 on various loss functions.

\begin{itemize}
    \item Best performance on balanced test data.
    \item Best performance on long tailed test data.
    \item Best performance on head classes.
    \item Best performance on tail classes.
    \item Worst performance on blananced test data.
    \item Worst performance on long tailed test data.
    \item Worst performance on head classes.
    \item Wrost performance on tail classes.
    \item Comment on the curious results on the middle classes.
\end{itemize}

\begin{table}[H]
    \centering
    \begin{tabular}{cccccc}
        \toprule
        Loss Function & Balanced & Long-tailed & Head & Middle & Tail \\ 
        \midrule
        Softmax loss   & 0.5620 & 0.5671 & 0.5521 & 0.6036 & 0.7368 \\
        Focal loss   & 0.5516 & 0.5538 & 0.5438 & 0.5680 & 0.7105 \\
        Weighted Softmax loss   & 0.5620 & 0.5671 & 0.5521 & 0.6036 & 0.7368 \\
        Class-balanced loss   & 0.5620 & 0.5671 &  0.5521 & 0.6036 & 0.7368 \\
        Balanced Softmax loss   & 0.5628 & 0.5642 & 0.5640 & 0.5325 & 0.7105 \\
        Equalization loss   & 0.5634   & 0.5519 & 0.5462 & 0.5503 & 0.6842 \\
        LDAM loss   & 0.5906 &  0.6013 & 0.5924 & 0.6095 & 0.7632 \\
        \bottomrule
    \end{tabular}
    \caption{Evaluation results for ViT-B/16 trained on the custom balanced dataset, showing Acc1.}
    \label{tab:vit_bal_acc1_1}
\end{table}

Table \ref{tab:vit_lt_acc1} show the top 1 accuracies for ViT-B/16 on various loss functions.

\begin{itemize}
    \item Best performance on balanced test data.
    \item Best performance on long tailed test data.
    \item Best performance on head classes.
    \item Best performance on tail classes.
    \item Worst performance on blananced test data.
    \item Worst performance on long tailed test data.
    \item Worst performance on head classes.
    \item Wrost performance on tail classes.
    \item Comment on the curious results on the middle classes.
\end{itemize}

\begin{table}[H]
    \centering
    \begin{tabular}{cccccc}
        \toprule
        Loss Function & Balanced & Long-tailed & Head & Middle & Tail \\ 
        \midrule
        Softmax loss   & 0.2254 & 0.4367 & 0.5071 & 0.1775 & 0.0263 \\
        Focal loss   & 0.2210 & 0.4206 & 0.4834 & 0.1953 & 0.0263 \\
        Weighted Softmax loss   & 0.1284 & 0.1760 & 0.1919 & 0.1302 & 0.0263 \\
        Class-balanced loss   & 0.0558 & 0.0076 & 0.0000 & 0.0237 & 0.1053 \\
        Balanced Softmax loss   & 0.2460 & 0.4244 & 0.4822 &  0.2130 & 0.0789 \\
        Equalization loss   & 0.2168 & 0.4215 & 0.4893 & 0.1716 & 0.0263 \\
        LDAM loss   & 0.1570 & 0.2750 & 0.3140 & 0.1361 & 0.0263 \\
        \bottomrule
    \end{tabular}
    \caption{Evaluation results for ViT-B/16 trained on the long-tailed dataset, showing Acc1.}
    \label{tab:vit_lt_acc1}
\end{table}

\subsubsection{Comparison}

TODO: Move comparisons of results to a seperate section. Keep only the results, put specification in an appendix.

\begin{table}[H]
    \centering
    \renewcommand{\arraystretch}{1.2} % Adjust row spacing
    \setlength{\tabcolsep}{4pt} % Adjust column spacing
    \begin{tabular}{lp{6cm}p{6cm}}
        \toprule
        \textbf{Aspect} & \textbf{Your Experiment} & \textbf{PUGD Results} \\ 
        \midrule
        Dataset           & CIFAR100 & CIFAR100 \\
        Model             & ViT-B/16 pretrained on ImageNet-21K & Multiple models: VGG-16, ResNet-18, DenseNet-121, UPANet-16, and ViT-B/16 \\
        Pretraining       & ImageNet-21K & ImageNet-1K (for fine-tuned models) \\
        Optimizer         & Adam & PUGD \\
        Loss Function     & Softmax Cross-Entropy & Softmax Cross-Entropy \\
        Epochs            & 90 & 200 (end-to-end); 80–100 (fine-tuning) \\
        Learning Rate     & Step decay: 0.001 → 0.0001 → 0.00001 & Cosine Annealing: 0.1 (end-to-end); 0.01–0.005 (fine-tuning) \\
        Augmentation      & Resize (224), RandomCrop, Horizontal Flip, Normalize & Resize (224), RandAugment, Cutout, Normalize \\
        Top-1 Accuracy    & - & End-to-End: Up to 78.30\% (DenseNet-121); Fine-tuning: ViT-B/16 achieves 93.95\% \\
        Hardware          & 4x NVIDIA TITAN X (Pascal, 12GB) & RTX-Titan, 32GB RAM, eight-core processor \\
        \bottomrule
    \end{tabular}
    \caption{Comparison of my experiment with PUGD results on CIFAR-100.}
    \label{tab:comparison3}
\end{table}


\subsection{ConvNeXt Base}

Table \ref{tab:conv_bal_acc1_1} show the top 1 accuracies for ConvNeXt Base on various loss functions.

\begin{itemize}
    \item Best performance on balanced test data.
    \item Best performance on long tailed test data.
    \item Best performance on head classes.
    \item Best performance on tail classes.
    \item Worst performance on blananced test data.
    \item Worst performance on long tailed test data.
    \item Worst performance on head classes.
    \item Wrost performance on tail classes.
    \item Comment on the curious results on the middle classes.
\end{itemize}

\begin{table}[h!]
    \centering
    \begin{tabular}{cccccc}
        \toprule
        Loss Function & Balanced & Long-tailed & Head & Middle & Tail \\ 
        \midrule
        Softmax loss   & 0.8332 & 0.8535 & 0.8566 & 0.8166 & 0.9474 \\
        Focal loss   & 0.8314 & 0.8487 & 0.8507 & 0.8284 & 0.8947 \\
        Weighted Softmax loss   & 0.8332 & 0.8535 & 0.8566 &  0.8166 & 0.9474 \\
        Class-balanced loss   & 0.8332 & 0.8535 & 0.8566 & 0.8166 & 0.9474 \\
        Balanced Softmax loss   & 0.8364 & 0.8344 & 0.8365 & 0.7988 & 0.9474 \\
        Equalization loss   & 0.8318 & 0.8468 & 0.8448 & 0.8343 & 0.9474 \\
        LDAM loss   & 0.8316 & 0.8373 & 0.8412 & 0.8047 & 0.8947 \\
        \bottomrule
    \end{tabular}
    \caption{Evaluation results for ConvNeXt Base trained on the custom balanced dataset, showing Acc1.}
    \label{tab:conv_bal_acc1_1}
\end{table}

Table \ref{tab:conv_lt_acc1_1} show the top 1 accuracies for ConvNeXt Base on various loss functions.

\begin{itemize}
    \item Best performance on balanced test data.
    \item Best performance on long tailed test data.
    \item Best performance on head classes.
    \item Best performance on tail classes.
    \item Worst performance on blananced test data.
    \item Worst performance on long tailed test data.
    \item Worst performance on head classes.
    \item Wrost performance on tail classes.
    \item Comment on the curious results on the middle classes.
\end{itemize}

\begin{table}[h!]
    \centering
    \begin{tabular}{cccccc}
        \toprule
        Loss Function & Balanced & Long-tailed & Head & Middle & Tail \\ 
        \midrule
        Softmax loss   & 0.5972 & 0.8316 & 0.8898 & 0.6568 & 0.3158 \\
        Focal loss   & 0.5938 & 0.8145 & 0.8685 & 0.6568 & 0.3158 \\
        Weighted Softmax loss   & 0.4090 & 0.6356 & 0.6848 & 0.4911 & 0.1842 \\
        Class-balanced loss   & 0.0142 & 0.0019 & 0.0000 & 0.0000 & 0.0526 \\
        Balanced Softmax loss   & 0.6460 & 0.8230 & 0.8685 & 0.6509 & 0.5789 \\
        Equalization loss   & 0.5956 & 0.8278 & 0.8768 & 0.6923 & 0.3421 \\
        LDAM loss   & 0.3770 & 0.5956 & 0.6445 & 0.4260 & 0.2632 \\
        \bottomrule
    \end{tabular}
    \caption{Evaluation results for ConvNeXt Basetrained on the long-tailed dataset, showing Acc1.}
    \label{tab:conv_lt_acc1_1}
\end{table}


% \section{Head, Middle, and Tail Class Performance}
% Break down the performance into head, middle, and tail class groups.
% Include visualizations.
% Discuss how well the methods balance performance across these groups, particularly focusing on tail classes.


\section{Comparison of Loss Functions}
Analyze how different loss functions impact performance.
Use visualizations to compare results.
Discuss strengths and weaknesses of each loss function.

\section{Comparison with Baselines}
Compare the results to the softmax cross-entropy loss.

\section{Comparison with External Findings}
Compare the results of baseline CIFAR100 trained on the models used in this thesis to state-of-the-art performance from paperswithcode.

\section{Qualitative Results}
Include if time.
Provide examples of correctly and incorrectly classified samples, especially for tail classes.
Include visualizations or images of difficult cases to highlight challenges in tail-class prediction.

\section{Summary and Discussion}
Recap the key findings, such as which methods or loss functions performed best and why.
Connect these findings to the thesis objectives and broader implications for long-tailed learning.