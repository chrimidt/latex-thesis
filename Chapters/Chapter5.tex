% Chapter 5: Results and Discussion
\label{chap:results}
This chapter presents the experimental results, comparing model performances across head, middle, and tail classes, and discusses the impact of different model architectures combined with different loss functions. First, the main findings are presented, followed by a detailed analysis of the results across experiments, and lastly a summary and discussion of the results. 

% \todo{Compare the different types of losses against each other. See \cite{menon2021longtaillearninglogitadjustment}.}

\section{Main Findings}
Across all models, BS loss demonstrated the highest performance on tail classes for models trained on long-tailed datasets while maintaining consistent performance on head, middle, and overall long-tailed test sets. The highest top-1 accuracy for tail classes was achieved by the ResNet-50 architecture (Acc1: 60.53\%), closely followed by the ConvNeXt-Base architecture (Acc1: 57.89\%). However, this improved tail-class performance comes at the cost of head-class accuracy, where ConvNeXt-Base outperforms ResNet-50 with a top-1 accuracy of 86.85\% compared to 82.70\%. Overall, ConvNeXt-Base demonstrates better performance across all classes (Acc1: 82.30\%) on BS loss compared to ResNet-50 (Acc1: 79.16\%). See Tables \ref{tab:resnet_lt_acc1_1} and \ref{tab:conv_lt_acc1_1} for reference. 

Remarkably, SEQL consistently underperformed when trained with ResNet-50, MobileNetV2, and ViT-B/16, but achieved comparably good results with ConvNeXt-Base on tail classes (Acc1: 47.37\%). Similarly, the ViT-B/16 architecture demonstrated the lowest overall accuracy when trained on both balanced and long-tailed datasets (Acc1: 59.06\%, see Table \ref{tab:vit_bal_acc1_1}), despite having the highest reported benchmark performance trained with balanced CIFAR-100 (Acc1: 93.95 \%) among all model architectures investigated in this thesis. This discrepancy suggests that the ViT-B/16 configuration used for the experiments in this thesis may not be well-suited for smaller-scale datasets such as CIFAR-100 without careful optimization as discussed in sections \ref{sec:ViTs} and \ref{sec:model_selection}.

\section{Overall Results}
This section presents the overall results of all experiments conducted in this thesis, highlighting the best and worst performance of each loss design on specific models without directly comparing losses or models. All models are first trained on a balanced CIFAR-100 dataset, to provide a reference point for evaluating the effectiveness of class re-balancing strategies. It provides an overview of all findings to understand how each combination of architecture and loss function behaves, starting with the ResNet-50 architecture.

\subsection{ResNet-50}

From Table \ref{tab:resnet_bal_acc1_1}, CE and WCE loss yield identical performance when trained on balanced data due to their cross-entropy term in equations \eqref{eq:cb_loss} and \eqref{eq:wce_loss}. BS loss demonstrates strong performance on the long-tailed test set (Acc1: 84.30\%) and head classes (Acc1: 84.60\%), while still performing comparably on tail classes. Notably, CE loss, WCE loss, and CB Loss all achieve top-1 accuracy of 94.74 \% on tail classes as the greatest performance of all loss designs. Overall, the performance of the class re-balancing methods compare to that of the CE baseline. The best performing loss design (CE) achieved and accuracy of 83.24\%, not far behind the best published result for ResNet-50 with an accuracy of 86.90\% as seen in table \ref{tab:benchmark_comparison}.

\begin{table}[h!]
    \centering
    \caption{Evaluation results for ResNet50 trained on the custom balanced dataset, showing Acc1.}
    \small
    \begin{tabular}{cccccc}
        \toprule
        Loss Function & Balanced & Long-tailed & Head & Middle & Tail \\ 
        \midrule
        Softmax loss   & \textbf{0.8324}  & 0.8421 & 0.8448 & 0.8047 & \textbf{0.9474} \\
        Focal loss   & 0.8310  & 0.8344 & 0.8341 & \textbf{0.8166} & 0.9211 \\
        Weighted Softmax loss   & \textbf{0.8324} & 0.8421 & 0.8448 & 0.8047 & \textbf{0.9474} \\
        Class-balanced loss   &  0.8144 & 0.8173 & 0.8199 & 0.7751 & \textbf{0.9474} \\
        Balanced Softmax loss   & 0.8310 & \textbf{0.8430} & \textbf{0.8460} & 0.8107 & 0.9211 \\
        Equalization loss   & 0.8316 & 0.8354 & 0.8365 & \textbf{0.8166} & 0.8947 \\
        LDAM loss   & 0.7990 & 0.7983 & 0.8069 & 0.7337 & 0.8947 \\
        \bottomrule
    \end{tabular}
    \label{tab:resnet_bal_acc1_1}
\end{table}

From Table \ref{tab:resnet_lt_acc1_1}, BS loss significantly improves tail-class accuracy (Acc1: 60.53\%), outperforming other loss designs, while also outperforming on balanced and middle sets. CE loss achieves the best performance on the long-tailed dataset and head classes, indicating that while Balanced Softmax improves minority-class accuracy, CE loss remains strong for majority classes. Comparing with the results form table \ref{tab:resnet_bal_acc1_1}, most losses exhibits strong performance on head classes, except for SEQL with an underwhelming performance across all categories. Likewise, LDAM loss struggles with most categories, and particularly with tail classes.

\begin{table}[h!]
    \centering
    \caption{Evaluation results for ResNet50 trained on the long-tailed dataset, showing Acc1.}
    \small
    \begin{tabular}{cccccc}
        \toprule
        Loss Function & Balanced & Long-tailed & Head & Middle & Tail \\ 
        \midrule
        Softmax loss   & 0.5522 & \textbf{0.7954} & \textbf{0.8531} & 0.6391 & 0.2105 \\
        Focal loss   & 0.5456 & 0.7935 & 0.8483 & 0.6272 & 0.3158 \\
        Weighted Softmax loss   & 0.4976 & 0.7336 & 0.7915 & 0.5562 & 0.2368 \\
        Class-balanced loss   & 0.4970 & 0.7450 & 0.8069 & 0.5562 & 0.2105 \\
        Balanced Softmax loss   & \textbf{0.5908} & 0.7916 & 0.8270 & \textbf{0.6568} & \textbf{0.6053} \\
        Equalization loss   & 0.0886 & 0.1399 & 0.1505 & 0.1124 &  0.0263 \\
        LDAM loss   & 0.3742 & 0.5937 & 0.6469 & 0.4438 & 0.0789 \\
        \bottomrule
    \end{tabular}
    \label{tab:resnet_lt_acc1_1}
\end{table}


\subsection{MobileNetV2}

\begin{table}[H]
    \centering
    \caption{Top-1 accuracy results for MobileNetV2 on the balanced dataset across all loss functions.}
    \small
    \begin{tabular}{cccccc}
        \toprule
        Loss Function & Balanced & Long-tailed & Head & Middle & Tail \\ 
        \midrule
        Softmax   & 0.7978   & \textbf{0.8059} & \textbf{0.8069} & \textbf{0.7870} & 0.8684 \\
        Focal loss   & 0.8014   & 0.8011 & 0.7998 & \textbf{0.7870} & 0.8947 \\
        Weighted Softmax loss   & 0.7978   & \textbf{0.8059} & \textbf{0.8069} & \textbf{0.7870} & 0.8684 \\
        Class-balanced loss   & 0.8008   & 0.8049 & 0.8140 & 0.7456 & 0.8684 \\
        Balanced Softmax loss   & 0.8034  & 0.8030 & \textbf{0.8069} & 0.7574 & \textbf{0.9211} \\
        Equalization loss   &  \textbf{0.8078}  & 0.7916 & 0.7962 & 0.7396 & \textbf{0.9211} \\
        LDAM loss   &  0.7828   & 0.7821 & 0.7808 & 0.7574 & \textbf{0.9211} \\
        \bottomrule
    \end{tabular}
    \label{tab:mobilenet_bal_acc1_1}
\end{table}

As shown in Table \ref{tab:mobilenet_bal_acc1_1}, SEQL achieves the highest accuracy on the balanced test dataset (Acc1: 80.78\%) and, along with BS loss and LDAM loss, yields the best performance on tail classes (Acc1: 92.11\%), despite differences in loss designs. Not surprisingly, CE loss and WCE loss yield the same accuracies across all test datasets due to shared cross-entropy designs, becoming indistinguishable when trained on balanced data, per equations \eqref{eq:ce_loss} and \eqref{eq:wce_loss}. Overall, all methods achieve results comparable to the CE baseline and surpass the best published performance for MobileNetV2 trained on CIFAR-100, as shown in Table \ref{tab:benchmark_comparison}.

\begin{table}[h!]
    \centering
    \caption{Top-1 accuracy results for MobileNetV2 on the long-tailed dataset across all loss functions.}
    \small
    \begin{tabular}{cccccc}
        \toprule
        Loss Function & Balanced & Long-tailed & Head & Middle & Tail \\ 
        \midrule
        Softmax   & 0.5282   & 0.7735 & 0.8341 & 0.5917 & 0.2368 \\
        Focal loss   & 0.5200   & \textbf{0.7745} & \textbf{0.8389} & 0.5917 & 0.1579 \\
        Weighted Softmax loss   & 0.5016   & 0.7231 & 0.7808 & 0.5503 & 0.2105 \\
        Class-balanced loss   &  0.5034  & 0.7336 & 0.7938 & 0.5385 & 0.2632 \\
        Balanced Softmax loss   & \textbf{0.5796}   & 0.7650 & 0.8069 & \textbf{0.6331} & \textbf{0.4211} \\
        Equalization loss   &   0.0618 & 0.0647 & 0.0675 & 0.0592 & 0.0263 \\
        LDAM loss   & 0.4264 & 0.5899 & 0.6137 & 0.5444 & 0.2632 \\
        \bottomrule
    \end{tabular}
    \label{tab:mobilenet_lt_acc1_1}
\end{table}

From Table \ref{tab:mobilenet_lt_acc1_1}, BS loss outperforms other losses in terms of balanced test accuracy (Acc1: 57.96\%) and especially tail classes (Acc1: 42.11\%). Although CE loss and FL surpass BS loss on the long-tailed dataset and head classes, the improvements BS loss provides for tail and middle classes are notable. SEQL shows an overwhelming underperformance compared with the balanced training in table \ref{tab:mobilenet_bal_acc1_1}, and now appears severely limited. The difference in performance on tail classes compared to the CE baseline shows that loss functions designed for imbalance can provide more nuanced performance gains.


\subsection{ViT-B/16}

From Table \ref{tab:vit_bal_acc1_1}, LDAM loss achieves the overall best performance for ViT-B/16 trained on balanced data. However, the overall performance remain far below the benchmark results for Vision Transformers on CIFAR-100, as seen in table \ref{tab:benchmark_comparison}. Although ViTs has achieved SOTA performance in image classification, their architecture differs from CNNs, as described in section \ref{sec:ViTs}, and may require careful fine-tuning for optimal results. 

\begin{table}[h!]
    \centering
    \caption{Evaluation results for ViT-B/16 trained on the custom balanced dataset, showing Acc1.}
    \small
    \begin{tabular}{cccccc}
        \toprule
        Loss Function & Balanced & Long-tailed & Head & Middle & Tail \\ 
        \midrule
        Softmax loss   & 0.5620 & 0.5671 & 0.5521 & 0.6036 & 0.7368 \\
        Focal loss   & 0.5516 & 0.5538 & 0.5438 & 0.5680 & 0.7105 \\
        Weighted Softmax loss   & 0.5620 & 0.5671 & 0.5521 & 0.6036 & 0.7368 \\
        Class-balanced loss   & 0.5432 & 0.5452 & 0.5427 & 0.5207 & 0.7105 \\
        Balanced Softmax loss   & 0.5628 & 0.5642 & 0.5640 & 0.5325 & 0.7105 \\
        Equalization loss   & 0.5582 & 0.5547 & 0.5462 & 0.5680 & 0.6842 \\
        LDAM loss   & \textbf{0.5906} &  \textbf{0.6013} & \textbf{0.5924} & \textbf{0.6095} & \textbf{0.7632} \\
        \bottomrule
    \end{tabular}
    \label{tab:vit_bal_acc1_1}
\end{table}


\begin{table}[h!]
    \centering
    \caption{Evaluation results for ViT-B/16 trained on the long-tailed dataset, showing Acc1.}
    \small
    \begin{tabular}{cccccc}
        \toprule
        Loss Function & Balanced & Long-tailed & Head & Middle & Tail \\ 
        \midrule
        Softmax loss   & 0.2254 & \textbf{0.4367} & \textbf{0.5071} & 0.1775 & 0.0263 \\
        Focal loss   & 0.2210 & 0.4206 & 0.4834 & 0.1953 & 0.0263 \\
        Weighted Softmax loss   & 0.1284 & 0.1760 & 0.1919 & 0.1302 & 0.0263 \\
        Class-balanced loss   & 0.1346 & 0.1865 & 0.2014 & 0.1361 & 0.0789 \\
        Balanced Softmax loss   & \textbf{0.2460} & 0.4244 & 0.4822 &  \textbf{0.2130} & 0.0789 \\
        Equalization loss   & 0.1686 & 0.2778 & 0.3140 & 0.1361 & \textbf{0.1053} \\
        LDAM loss   & 0.1570 & 0.2750 & 0.3140 & 0.1361 & 0.0263 \\
        \bottomrule
    \end{tabular}
    \label{tab:vit_lt_acc1}
\end{table}

The results from Table \ref{tab:vit_lt_acc1} show low performance across all loss designs. BS loss slightly improves results on balanced and middle categories, while CE oss achieves the highest accuracy on the long-tailed dataset and head classes. The best performance on tail classes (Acc1: 10.53\%) is achieved by SEQL, however, still unsatisfactory. This pattern suggests that the ViT-B/16 architecture may not be sufficient for robust performance on smaller-scale, long-tailed datasets and may require furhter adjustments in the training pipeline, as outlined in section \ref{sec:ViTs}. 

\subsection{ConvNeXt Base}
From Table \ref{tab:conv_bal_acc1_1}, CB loss achieves the highest balanced test accuracy of 83.64\% while the CE loss, WCE loss, and BS loss tie for top performance on the long-tailed dataset (Acc1: 94.74\%). CE loss and WCE loss yield same perfromance across all categories due to their shared design on balanced datasets. The saturation of tail-class performance is likely due to the quality or quantity of tail classes. For comparison, the best published result for a ConvNext architecture trained with CIFAR-100 achieve an accuarcy of 94.04\% (see table \ref{tab:benchmark_comparison}), the best performance of on the balanced test set is in alignment yields an accuracy of 83.98\%, implying room for improvement in the training pipeline.

\begin{table}[h!]
    \centering
    \caption{Evaluation results for ConvNeXt Base trained on the custom balanced dataset, showing Acc1.}
    \small
    \begin{tabular}{cccccc}
        \toprule
        Loss Function & Balanced & Long-tailed & Head & Middle & Tail \\ 
        \midrule
        Softmax loss   & 0.8332 & \textbf{0.8535} & \textbf{0.8566} & 0.8166 & \textbf{0.9474} \\
        Focal loss   & 0.8314 & 0.8487 & 0.8507 & \textbf{0.8284} & 0.8947 \\
        Weighted Softmax loss   & 0.8332 & \textbf{0.8535} & \textbf{0.8566} &  0.8166 & \textbf{0.9474} \\
        Class-balanced loss   & \textbf{0.8398} & 0.8344 & 0.8389 & 0.7929 & 0.9211 \\
        Balanced Softmax loss   & 0.8364 & 0.8344 & 0.8365 & 0.7988 & \textbf{0.9474} \\
        Equalization loss   & 0.8322 & 0.8392 & 0.8412 & 0.8107 & 0.9211 \\
        LDAM loss   & 0.8316 & 0.8373 & 0.8412 & 0.8047 & 0.8947 \\
        \bottomrule
    \end{tabular}
    \label{tab:conv_bal_acc1_1}
\end{table}


\begin{table}[h!]
    \centering
    \caption{Evaluation results for ConvNeXt Basetrained on the long-tailed dataset, showing Acc1.}
    \small
    \begin{tabular}{cccccc}
        \toprule
        Loss Function & Balanced & Long-tailed & Head & Middle & Tail \\ 
        \midrule
        Softmax loss   & 0.5972 & \textbf{0.8316} & \textbf{0.8898} & \textbf{0.6568} & 0.3158 \\
        Focal loss   & 0.5938 & 0.8145 & 0.8685 & \textbf{0.6568} & 0.3158 \\
        Weighted Softmax loss   & 0.4090 & 0.6356 & 0.6848 & 0.4911 & 0.1842 \\
        Class-balanced loss   & 0.3436 & 0.5756 & 0.6469 & 0.3254 & 0.1053 \\
        Balanced Softmax loss   & \textbf{0.6460} & 0.8230 & 0.8685 & 0.6509 & \textbf{0.5789} \\
        Equalization loss   & 0.5948 & 0.7973 & 0.8460 & 0.6272 & 0.4737 \\
        LDAM loss   & 0.3770 & 0.5956 & 0.6445 & 0.4260 & 0.2632 \\
        \bottomrule
    \end{tabular}
    \label{tab:conv_lt_acc1_1}
\end{table}

Table \ref{tab:conv_lt_acc1_1} shows that BS loss significantly outperforms other methods on the balanced test set (Acc1: 64.60\%) and tail classes (Acc1: 57.89\%). CE loss achieves the best performance on the long-tailed dataset, as well as on head and middle classes. Notably, SEQL delivers comparable results across all categories, in contrast to its performance with other model architectures, where it fell short compared to other loss designs.

\section{Benchmarks}
Table \ref{tab:benchmark_comparison} compares the best results from the models trained on the balanced CIFAR-100 dataset in this thesis with published benchmarks using the same backbone architectures. This comparison serves as a reference to ensure the results align with expectations and to identify any potential discrepancies. 

For MobileNetV2, the results in this thesis surpass the published benchmarks. This could be attributed to improved loss function designs. On the other hand, both ResNet-50 and ConvNeXt-Base exhibits lower accuracies compared to benchmarks, which may be explained by differences in training pipelines.

The most significant deviation is observed with ViT-B/16, where the accuracy falls far short of the benchmark. This discrepancy suggests that the configuration for ViT-B/16 may not optimized for training with small-scale datasets, as outlined in section \ref{sec:ViTs}. 

Overall, these results indicate that the experimental setup is effective for most models but highlights potential limitations for transformer-based architectures such as ViT-B/16. Future investigation could lead to an understanding of these discrepancies.

\begin{table}[H]
    \centering
    \caption{Comparison of model performance (top-1 accuracy) with published benchmarks. The models are trained and tested on a balanced CIFAR-100 dataset. The performances are achieved with varying the techniques, and are meant as a reference and not a direct comparison.}
    \small
    \begin{tabular}{lcc}
    \toprule
    \textbf{Model}        & \textbf{Result (Acc1)} & \textbf{Benchmark Result (Acc1)}  \\ 
    \midrule
    ResNet-50            & 83.24\%               & 86.90\% \cite{wightman2021resnetstrikesbackimproved}  \\ 
    MobileNetV2           & 80.78\%               & 73.20\% \cite{park2022bitatneuralnetworkbinarization}  \\ 
    ViT-B/16              & 59.06\%               & 93.51\% \cite{ye2023partialfinetuningsuccessorfinetuning} \\ 
    ConvNeXt-Base         & 83.98\%               & 94.04\% \cite{ye2023partialfinetuningsuccessorfinetuning}  \\ 
    \bottomrule
    \end{tabular}
    \label{tab:benchmark_comparison}
\end{table}



\section{Performance Comparisons}
\subsection{Model Performance}
The models are evaluated based on the mean and standard deviation of their results across all loss functions. These statistics are visualized in Figure \ref{fig:mean_loss_comparison} and detailed in Table \ref{tab:model_performance_summary}. Here, the performances of the models MobileNetV2, ResNet-50, ViT-B/16, and ConvNeXt-Base are shown across the evaluation categories: Balanced, Long-Tailed, Head, Middle, and Tail. Each model is slightly offset along the x-axis to avoid overlap. Each point represents the mean accuracy of a model for a specific category and the bars represent the standard deviation. The error bars indicate the variability in accuracy across different loss functions for each model and category. A longer error bar demonstrate a less consistent performance, dependent on the chosen loss function, while shorter error bars indicate consistant performance adhering to the specific model.

Ignoring the sub-par performance of the ViT-B/16 architecture, the shorter errorbars for ConvNeXt-Base in figure \ref{fig:mean_loss_comparison} indicate that its architecture may be more robust to class-sensitive strategies than other architechtures. Additionally, this architecture displays the greatest mean accuracies across all categories. Contrary, the performance of MobileNetV2 shows a greater sensitivity to the choice of loss design with the exception of ResNet-50 on tail classes. ViT-B/16 shows a consistently lower mean accuracy, indicating that transformer-based approaches may require additional adaptations for smaller-scale or imbalanced datasets, as discussed in \ref{sec:model_selection}. Overall, ConvNeXt-Base maintains strong and stable performance across all categories, benefiting from BS loss as seen in table \ref{tab:conv_lt_acc1_1}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{Images/Plots/mean_loss_comparison.png}
    \caption{Mean Accuracy with Standard Deviation Across Categories for MobileNetV2, ResNet-50, ViT-B/16, and ConvNeXt Base trained with CIFAR-100-LT.}
    \label{fig:mean_loss_comparison}
\end{figure}

\begin{table}[h!]
    \centering
    \caption{Performance Summary Across Categories (Mean ± Std)}
    \scriptsize
    \begin{tabular}{lccccc}
        \toprule
        Model & Balanced & Long-tailed & Head & Middle & Tail \\
        \midrule
        MobileNetV2 
        & 0.4459 ± 0.1623 & 0.6320 ± 0.2391 & 0.6765 ± 0.2585 & 0.5013 ± 0.1832 & 0.2256 ± 0.1107 \\
        ResNet-50 
        & 0.4494 ± 0.1605 & 0.6561 ± 0.2207 & 0.7035 ± 0.2348 & 0.5131 ± 0.1767 & 0.2406 ± 0.1743 \\
        ViT-B/16 
        & 0.1830 ± 0.0438 & 0.3139 ± 0.1047 & 0.3563 ± 0.1250 & 0.1606 ± 0.0315 & 0.0526 ± 0.0315 \\
        ConvNeXt-Base 
        & 0.5088 ± 0.1170 & 0.7247 ± 0.1077 & 0.7784 ± 0.1050 & 0.5477 ± 0.1243 & 0.3196 ± 0.1505 \\
        \bottomrule
    \end{tabular}
    \label{tab:model_performance_summary}
\end{table}

\subsection{Loss Comparison}
The BS loss emerges as the overall best performing loss design across all models with the least variance in performance across test categories, excluding the performances of ViT-B/16, as figures \ref{fig:resnet_lt_loss_comparison}, \ref{fig:mobilenet_lt_loss_comparison}, and \ref{fig:conv_lt_loss_comparison} illustrate. For reference, the performance of ViT-B/16 is illustrated in figure \ref{fig:vit_lt_loss_comparison}. From figure \ref{fig:conv_lt_loss_comparison}, the SEQL stands out in performance across all categories, and nearly competes with BS loss in terms of variance. In contrast, SEQL exhibits the worst performance on both ResNet-50 and MobileNetV2 (see Figures \ref{fig:resnet_lt_loss_comparison} and \ref{fig:mobilenet_lt_loss_comparison}), possibly because these architectures are less robust to the large negative logits and masking strategy enforced by SEQL as opposed to ConvNeXt-Base.

These result are further underlined in figure \ref{fig:loss_comparison_bars}, where the mean and standard deviation of the loss functions across the four backbone architectures are illustrated. Here, the BS loss shows a better average perfomance on tail classes, while also displaying the smoothest variation across evaluation categories, ignoring the SEQL. Additionally, the standard deviation suggests that the perfomance of the BS loss on tail classes depend on the model architecture. Table \ref{tab:performance_summary} display the exact values of the mean and standard deviations shown in figure \ref{fig:loss_comparison_bars}. 

% \begin{figure}[ht]
%     \centering
%     \begin{subfigure}{0.75\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Images/resnet_lt_loss_comparison_copy.png}
%         \caption*{(a)}
%         \label{fig:resnet_lt_loss_comparison}
%     \end{subfigure}
%     \vspace{1em} % Add some vertical spacing

%     \begin{subfigure}{0.75\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Images/mobilenet_lt_loss_comparison_copy.png}
%         \caption*{(b)}
%         \label{fig:mobilenet_lt_loss_comparison}
%     \end{subfigure}
%     \vspace{1em} % Add some vertical spacing

%     \begin{subfigure}{0.75\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Images/vit_lt_loss_comparison_copy.png}
%         \caption*{(c)}
%         \label{fig:vit_lt_loss_comparison}
%     \end{subfigure}
%     \vspace{1em} % Add some vertical spacing

%     \begin{subfigure}{0.75\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Images/Plots/convnext_lt_loss_comparison.png}
%         \caption*{(d)}
%         \label{fig:conv_lt_loss_comparison}
%     \end{subfigure}

%     \caption{Comparison of top 1 accuracy across test categories (Balanced, Long-tailed, Head, Middle, Tail) for different loss functions on various models.}
%     \label{fig:lt_loss_comparison_all_models}
% \end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{Images/Plots/resnet_lt_loss_comparison.png}
    \caption{ResNet-50 top 1 accuracy across test categories (Balanced, Long-tailed, Head, Middle, Tail) for different loss functions.}
    \label{fig:resnet_lt_loss_comparison}
\end{figure}
\FloatBarrier

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{Images/Plots/mobilenet_lt_loss_comparison.png}
    \caption{MobileNetV2 top 1 accuracy across test categories (Balanced, Long-tailed, Head, Middle, Tail) for different loss functions.}
    \label{fig:mobilenet_lt_loss_comparison}
\end{figure}
\FloatBarrier

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{Images/Plots/vit_lt_loss_comparison.png}
    \caption{ViT-B/16 top 1 accuracy across test categories (Balanced, Long-tailed, Head, Middle, Tail) for different loss functions.}
    \label{fig:vit_lt_loss_comparison}
\end{figure}
\FloatBarrier

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{Images/Plots/convnext_lt_loss_comparison.png}
    \caption{ConvNeXt-Base top 1 accuracy across test categories (Balanced, Long-tailed, Head, Middle, Tail) for different loss functions.}
    \label{fig:conv_lt_loss_comparison}
\end{figure}
\FloatBarrier


\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{Images/Plots/loss_function_bar_plot_mean_std.png}
    \caption{Performance trends of different loss functions across evaluation categories. Error bars indicate the standard deviation of accuracy across models, highlighting variability in performance for each loss function.}
    \label{fig:loss_comparison_bars}
\end{figure}
\FloatBarrier


\begin{table}[h!]
    \centering
    \caption{Performance Summary Across Categories (Mean ± Std)}
    \scriptsize
    \begin{tabular}{lccccc}
        \toprule
        Loss Function & Balanced & Long-tailed & Head & Middle & Tail \\
        \midrule
        Balanced Softmax 
        & $0.5156 \pm 0.1577$ & $0.7010 \pm 0.1610$ & $0.7462 \pm 0.1540$ & $0.5385 \pm 0.1881$ & $0.4211 \pm 0.2097$ \\
        Weighted Softmax 
        & $0.3842 \pm 0.1522$ & $0.5671 \pm 0.2290$ & $0.6123 \pm 0.2462$ & $0.4320 \pm 0.1761$ & $0.1645 \pm 0.0819$ \\
        Softmax 
        & $0.4758 \pm 0.1466$ & $0.7093 \pm 0.1587$ & $0.7710 \pm 0.1537$ & $0.5163 \pm 0.1970$ & $0.1974 \pm 0.1061$ \\
        Focal Loss 
        & $0.4701 \pm 0.1462$ & $0.7008 \pm 0.1624$ & $0.7598 \pm 0.1599$ & $0.5178 \pm 0.1876$ & $0.2039 \pm 0.1211$ \\
        Class-balanced 
        & $0.3697 \pm 0.1500$ & $0.5602 \pm 0.2259$ & $0.6123 \pm 0.2454$ & $0.3891 \pm 0.1720$ & $0.1645 \pm 0.0753$ \\
        LDAM 
        & $0.3337 \pm 0.1041$ & $0.5136 \pm 0.1377$ & $0.5548 \pm 0.1396$ & $0.3876 \pm 0.1520$ & $0.1579 \pm 0.1069$ \\
        Equalization 
        & $0.2285 \pm 0.2151$ & $0.3199 \pm 0.2860$ & $0.3445 \pm 0.3028$ & $0.2337 \pm 0.2289$ & $0.1579 \pm 0.1852$ \\
        \bottomrule
    \end{tabular}
    \label{tab:performance_summary}
\end{table}



%\section{Summary and Discussion}
% The overall best performance on tail classes was achieved by the Balanced Softmax Loss, which proved to be consistent across all models while also achieving competing results on other test categories. Furthermore, these findings highlight the impact of model architecture combined with loss designs, as the standard deviation demonstrates when comparing figures \ref{fig:mean_loss_comparison} and \ref{fig:loss_comparison_bars}. Here, the longer errorbars suggests the dependency of loss design and model architecture, respectively. Further investigation of model and loss design combinations could lead to greater tail-class performance.

% Although the BS Loss' overall superior performance on tail classes, this performance accuracy often came with trade-offs in performance on head classes. For example, while ResNet-50 achieved the highest top 1 accuracy of 60.53\% on tail classes using Balanced Softmax, compared to that of ConvNeXt Base (Acc1: 57.89\%), head-class accuracy (82.70\%) lagged behind ConvNeXt Base (86.85\%). This trade-off highlights the challenge of optimizing for both head and tail classes simultaneously and indicates that Balanced Softmax prioritizes tail-class adjustments at the expense of head-class performance. 

% SEQL, in contrast, consistently underperformed across all models and test categories with the exception of ConvNeXt-Base. This suggests that the down-weighting of the negative gradients posed by the SEQL affect the model's ability to distinguish classes from one another, as discussed in section \ref{sec:loss_selection}. The ConvNetxt architecture includes designs components that could help mitigate the performance drop imposed by the heavy masking, such as LayerNorm (see section \ref{sec:convnext}). When training on a small datasets like the long-tailed CIFAR-100, the use of SEQL could result in the model not receiving an adequate number of samples for training when negative gradients are down-weighted. Moreover, the model fails to learn fine-grained distinctions necessary to achieve high discriminative abilities.  

% Likewise, the ViT-B/16 architecture underperformed significantly across all loss designs (including the softmax baseline), categories, and training datasets, with the best accuracy of 59.06\% on a balanced training dataset compared to its benchmark of 93.95\% \cite{ye2023partialfinetuningsuccessorfinetuning}. These results indicate that the default ViT-B/16 architecture may not be well-suited for long-tailed datasets without further optimization \cite{menon2021longtaillearninglogitadjustment,loshchilov2018fixing}. These could include a carefully chosen optimizer, longer training, or larger dataset \cite{beyer2022betterplainvitbaselines}. Comparatively, the CNN architectures displayed overall better performance with the same setup. 

% The high and equal tail-class performances of ResNet-50, MobileNetV2, and ConvNeXt-Base trained on the balanced CIFAR-100 (see tables \ref{tab:resnet_bal_acc1_1}, \ref{tab:mobilenet_bal_acc1_1}, and \ref{tab:conv_bal_acc1_1}) suggests that tail-class performance may have reached a saturation point due to the limited number of samples in tail classes. These limitations could be investigated through testing on a larger datasets. 

% \paragraph{Optimizer Choice in Long-Tailed Learning}
% Studies has shown that different optimizers can influence performance in long-tailed classification due to an implicit bias \cite{soudry2024implicitbias,menon2021longtaillearninglogitadjustment}. Specifically, the Adam optimizer exhibits a weak correlation between class frequency and weight norms compared to SGD, which can result in different trade-offs when balancing head and tail performance \cite{menon2021longtaillearninglogitadjustment}. Adam was chosen primarily for its faster convergence and minimal hyperparameter tuning. However, future research could explore whether switching to SGD might yield further improvements in tail-class accuracy, especially when combined with re-weighting schemes or alternative loss functions.

% The performance of the LDAM could potentially improve by including the deferred re-weighting scheme (DRW), also introduced by Cao et al. \cite{cao2019learningimbalanceddatasetslabeldistributionaware}. According to related work \cite{menon2021longtaillearninglogitadjustment}, the DRW significanlty improves performance on long-tailed datasets in contrast to relying solely on LDAM for effective class re-balancing.

% The decision to derive test data from the training set, rather than splitting the validation set to match the class distribution to that of the ImageNet-LT (see section \ref{sec:dataset_specs}), could also influence the results, since the training data would include more samples. Likewise, the performance of the combinations of model architectures and loss designs could benefit from training with a larger dataset, like ImageNet and ImageNet-LT. Furthermore, these larger long-tailed benchmarks follow the Pareto distribution rather than an exponential decay, mirroring a natural occuring long-tailed dataset, as discussed in sections \ref{sec:lt-datasets} and \ref{sec:dataset_specs}.  


% Overall, these findings emphasize the importance of aligning loss functions with both dataset characteristics and model architectures to address the challenges of deep long-tailed learning. They also highlight the need for a nuanced approach that balances improvements in tail-class accuracy with minimal trade-offs in head-class and overall performance.

\section{Summary and Discussion}

\paragraph{Balanced Softmax and Tail-Class Performance}
By a large margin, the overall best performance on tail classes was achieved by the BS loss, which proved to be consistent across all models while also achieving competitive results on other test categories. BS works by shifting logits to benefit minority classes, effectively countering the natural bias toward majority classes. However, as discussed below, this re-scaling can sometimes come at the expense of head-class performance.

\paragraph{Head vs. Tail Performance Trade-Off}
Although BS loss excels at improving tail-class accuracy, this improvement often comes with a trade-off in head-class performance. For example, while ResNet-50 achieved the highest top-1 accuracy of 60.53\% on tail classes using BS (compared to 57.89\% on ConvNeXt-Base), its head-class accuracy (82.70\%) lagged behind that of ConvNeXt-Base (86.85\%). This trade-off highlights the challenge of optimizing for both head and tail classes simultaneously, indicating that BS loss prioritizes boosting tail classes more aggressively. Future work could explore strategies to fine-tune or balance this trade-off.


\paragraph{SEQL}
In contrast, SEQL consistently underperformed across most models and test categories, with the exception of ConvNeXt-Base. A possible reason is that SEQL down-weights negative gradients, which can inadvertently weaken the model's ability to develop fine-grained distinctions among classes (see Section~\ref{sec:seql}). The ConvNeXt architecture includes design elements such as LayerNorm (see Section~\ref{sec:convnext}) that might mitigate the adverse effects of heavy gradient masking. However, on small datasets like long-tailed CIFAR-100, down-weighting negative gradients deprives the model of critical examples for training, further impairing the learning of subtle class distinctions.

\paragraph{LDAM}
The LDAM loss could potentially be improved by combining it with a deferred re-weighting scheme (DRW), as introduced by Cao et al.~\cite{cao2019learningimbalanceddatasetslabeldistributionaware}. With DRW, the model applies class-weighting only after a fixed number of epochs. Related work~\cite{menon2021longtaillearninglogitadjustment} demonstrates that DRW significantly boosts performance on long-tailed datasets, in contrast to using LDAM alone.

\paragraph{CNNs vs.\ ViTs}
CNN architectures (ResNet-50, MobileNetV2, and ConvNeXt-Base) generally displayed better performance under the setup employed in this thesis. In contrast, ViT-B/16 underperformed significantly across all loss designs, categories, and training datasets, with its best accuracy reaching only 59.06\% on a balanced training dataset, compared to a benchmark of 93.95\%~\cite{ye2023partialfinetuningsuccessorfinetuning}. This discrepancy suggests that the default ViT-B/16 architecture may not be well-suited for smaller or long-tailed datasets without further adaptation. Transformers typically excel with large-scale, balanced data; hence, additional optimization techniques (e.g., specialized data augmentations, longer training, or advanced regularization) may be necessary to exploit ViT-B/16's full potential~\cite{menon2021longtaillearninglogitadjustment,loshchilov2018fixing}.

\paragraph{Saturation on Balanced CIFAR-100}
ResNet-50, MobileNetV2, and ConvNeXt-Base all displayed high, nearly equal tail-class performances when trained on the balanced version of CIFAR-100 (see Tables~\ref{tab:resnet_bal_acc1_1}, \ref{tab:mobilenet_bal_acc1_1}, and \ref{tab:conv_bal_acc1_1}). This suggests that tail-class accuracy may have reached a saturation point due to the inherently limited number of samples in these minority classes. Testing on larger, more diverse datasets would help clarify whether these accuracies can be further improved.

\paragraph{Optimizer Choice in Long-Tailed Learning}
Studies have shown that optimizer choice can influence long-tailed classification performance due to implicit biases \cite{soudry2024implicitbias,menon2021longtaillearninglogitadjustment}. Specifically, SGD with momentum tends to correlate more strongly with class frequency, amplifying the advantage of head classes. In contrast, Adam exhibits a weaker correlation between class frequency and weight norms, potentially influencing re-weighting schemes~\cite{menon2021longtaillearninglogitadjustment}.

Given these nuances, Adam was chosen primarily for its faster convergence and minimal hyperparameter tuning in this study. However, future work could explore switching to SGD to see if this yields better overall performance. For instance, a carefully tuned SGD might further improve tail-class accuracy when combined with a re-weighting strategy (like DRW).

\paragraph{Dataset Considerations}
The decision to derive test data directly from the training set, rather than creating a separate validation set with a matched long-tailed distribution (see Section~\ref{sec:dataset_specs}), may have influenced the reported results.

Likewise, the combinations of model architectures and loss designs could benefit from training on larger long-tailed benchmarks like ImageNet-LT. These larger datasets follow a more natural Pareto distribution rather than an exponential decay, better mirroring real-world scenarios (see Sections~\ref{sec:lt-datasets} and~\ref{sec:dataset_specs}). However, scaling to ImageNet-LT also introduces higher computational and resource demands.

\paragraph{Fine-Tuning and Transfer Learning}
Transfer learning from large-scale datasets (e.g., ImageNet) is widely acknowledged as a means of mitigating data scarcity in long-tailed problems~\cite{cs231n,ye2023partialfinetuningsuccessorfinetuning,kandel2020}. In this study, all layers of the pretrained models were fine-tuned rather than freezing any subset of them. Fine-tuning the entire network allows the model to fully adapt its learned features to the CIFAR-100 dataset, potentially benefiting minority classes that differ significantly from ImageNet classes~\cite{ye2023partialfinetuningsuccessorfinetuning}.

However, this comprehensive fine-tuning introduces two notable trade-offs. First, updating all layers raises the risk of overfitting~\cite{cs231n}, especially when dealing with a smaller dataset such as CIFAR-100. Second, because the lower layers often capture general, transferable features (e.g., edges, shapes), fully retraining these layers can lead to a loss of these broad representations~\cite{cs231n,yosinski2014transferablefeaturesdeepneural}. In future work, partial freezing or layer-wise fine-tuning could be investigated to preserve the generality of early-stage features while still adapting higher layers to the characteristics of the long-tailed CIFAR-100 distribution.



% \subsection{Future Directions}
% \paragraph{Balancing Head and Tail Classes More Effectively.}
% Finding the right equilibrium between head-class and tail-class accuracy remains a core challenge. Future work could explore adaptive or dynamic adjustments to Balanced Softmax or CB Loss, where re-scaling factors are periodically recalculated based on per-class performance.

% \paragraph{Combining LDAM with DRW.}
% As noted, \textbf{LDAM with DRW} may significantly improve performance. Implementing this combination in tandem with CNN and transformer architectures—possibly using different optimizers—could clarify the interplay between re-weighting strategies, loss functions, and architecture-specific inductive biases.

% \paragraph{Architectural and Optimizer Innovations.}
% Novel network architectures, specialized data augmentations, and flexible optimizers (e.g., AdaBound or Lion) might offer further gains. Thorough hyperparameter tuning, especially in learning rate schedules for both head and tail classes, can be essential for maximizing performance on imbalanced datasets.

% \paragraph{Larger, More Realistic Benchmarks.}
% Finally, testing these methods on larger, naturally imbalanced datasets would help validate whether the improvements seen on CIFAR-100 are robust in more complex, real-world conditions. The use of large-scale, long-tailed benchmarks—while computationally intensive—provides a more realistic test bed for practical applications.

% \subsection{Overall Conclusions}
% In summary, our findings demonstrate that:
% \begin{itemize}
%     \item \textbf{Loss Design Matters}: Balanced Softmax and CB Loss excel at boosting minority classes but may reduce head-class accuracy.  
%     \item \textbf{Model Architecture Impacts Performance}: CNNs generally handle imbalance better under our setup than ViT-B/16, which may require larger datasets or specialized tuning.  
%     \item \textbf{Optimizer Bias is Real}: Adam’s weaker correlation between class frequency and weight norms can work synergistically with re-weighting methods, though SGD might still outperform it under careful tuning.  
%     \item \textbf{Methodological Details Influence Outcomes}: Using training data for testing or employing small datasets can mask or exaggerate improvements.
% \end{itemize}

% Overall, these findings emphasize that tackling long-tailed learning requires aligning \emph{all} components—loss function, architecture, optimizer, and dataset design—to address the unique challenges posed by highly imbalanced data. Achieving gains in tail-class performance must be balanced with minimal trade-offs in head-class and overall accuracy, a tension likely best resolved through tailored or hybrid strategies that adapt to the specific data distribution at hand.
