% Chapter 1: Introduction

% Length: aim for 2-3 pages.

\label{sec:introduction}

\todo{Emphasize head and tail classes.}

Image classification is one of the main challenges computer vision.

Deep learning has become a prominent solution in recent years for tackling image regocnision tasks. With the availability of large datasets, i.e. ImageNet, along with GPUs, training of deep learning models have become easier, and have led to remarkable results \todo{reference here}. The trained models have shown image classification with accuracies of over 80 \% \todo{reference here}, and hence the interest in deep learning for image recognition is high. However, the high accuracies are for on models trained on balanced datasets with thounsands of samples per class \todo{reference here}, while most real-world datasets are skewed in samples per class \todo{reference here}.  


This thesis focuses on the problem with long-tailed datasets. The problem with training a deep learning model on long-tailed datasets is that the model will effectively the data from the classes with most samples, and not the classes with few samples. The finsihed model will then not recognize an input from the tail classes. Most real-world datasets follows a long-tailed structure, hence the need for a reliable method to detect examples of tail-class data. The aim of this thesis is to try out some of the methods tackling the long-tailed problem for deep learning described in the paper \textit{Deep Long-Tailed Learning: A Survey} by Zhang et al.\cite{zhang2023deep} to find a method for long-tailed learning that works on a specific long-tailed dataset of images of moths taken around equator. The goal of the moth dataset is to identify species.

\section{Problem Definition}
\todo{Something about head and tail classes.}
% Let {xi
% , yi}
% n
% i=1 be the long-tailed training set, where
% each sample xi has a corresponding class label yi
% . The total
% number of training set over K classes is n =
% PK
% k=1 nk, where
% nk denotes the data number of class k; let π denote the vector of
% label frequencies, where πk = nk/n indicates the label frequency
% of class k. Without loss of generality, a common assumption in
% long-tailed learning [31], [32] is that the classes are sorted by
% cardinality in decreasing order (i.e., if i1 < i2, then ni1 ≥ ni2
% ,
% and n1 nK), and then the imbalance ratio is defined as n1/nK.

The goal of this project is to investigate deep learning models, like Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs), and methods, mainly class re-balancing through cost-sensitive learning, and analyze their performance on datasets representaing a long-tailed structure.  

Four models are investigated, where three of them are CNNs, and one is a ViT. The CNNs investigated are the MobileNetV2, ResNet50v2, and ConvNeXt Base, while the ViT investigated is the ViT-B/16. All four models are pretrained on ImageNet, and further trained on both a balanced version of CIFAR100 and an long-tailed version of CIFAR100, called CIFAR100-LT, and introducing the long-tailed technique as class re-balancing in the loss functions by using re-weighting. All models are trained with Softmax Cross-Entropy Loss, Weighted Softmax Cross-Entropy Loss, Focal Loss, Class-Balanced Loss, Balanced Softmax Loss, LDAM Loss, and Equalization Loss. The main purpose of this master's thesis is to compare class re-balancing methods on a well respresenting type of models used in deep learning image recognition tasks. Furhter comparison with state-of-the-art methods for deep long-tailed learning will be made. The motivation for this comparison is to find a way to train on a real-world long-tailed dataset of moths to correctly identify species.


% These particular models were chosen because of their individual strenghts MobileNetV2 was designed for resource effeciency, respresenting a models designed for environments with limited resources. ResNet50v2 is build on the succes of the ResNet architecture, offering a strong baseline, providing a standard for comparison. ConvNeXt Base is a newer interpretation of convolutional neural networks, incorporating design principles of vision transformers, bridging the gap between tradional CNNs and ViTs. ViT-B/16 is 


\subsection{Goals of this thesis}
The goals of this thesis are to:

\begin{enumerate}
    \item Investigate the efficacy of long-tailed learning methods by assessing their performance on tail classes without sacrificing accuracy on head classes. 
    \item Understand how model design affects the performance of deep long-tailed learning methods.
    \item Provide a comprehensive insight in comparisons that inform the choice of methods for long-tailed distributions in acedemic and industry settings.
\end{enumerate}

\subsection{Hypothesis}
Deep long-tailed learning methods, such as the carefully designed loss functions tailored for long-tailed ditributed datasets, can improve the performance of underrepresented (tail) classes while maintaining the overall accuracy across diverse model architectures. The effectiveness of these loss functions is influenced by the choise of model architecture and the degree of imbalance of the dataset.


\subsection{Approach}
\todo{finish this section.}
This master's thesis consists of six steps, described below:\\  

\textbf{First step} is to investigate the dataset used for training, testing and validation in \textit{Deep Long-Tailed Learning: A Survey}, as the class re-balancing in this paper is used as inspiration for this thesis.\\

\textbf{Second step} is generating a long-tailed version of CIFAR100 that can be used for training and comparisons of methods.\\

\textbf{Third step} is the implementation of models and methods, combining them.\\

\textbf{Fourth step} is hyperparameter settings.\\

\textbf{Fifth step} is training and evaluation of the models with different loss functions. The training is both on the balanced and long-tailed version of CIFAR100.\\

\textbf{Sixth step} is a comparison of the methods.

Other steps could involve comparisons to related work and other long-tailed learning methods.

\subsection{Scope of this thesis}
This thesis focuses on applying deep long-tailed learning methods to image classification tasks with an emphasis on loss re-weighting as a solution to handle imbalanced datasets. The loss re-weighting methods include cross-entropy loss, focal loss, weighted cross-entropy loss,  class-balanced loss, balanced softmax loss, equalization loss, and LDAM loss. The experiments are conducted on the CIFAR100 dataset, with both a balanced and synthetically generated long-tailed version. The evaluation is done with particular focus on top-1 accuracy, with attention paid to performance on head, middle, and tail classes. This thesis does not explore other long-tailed learning approaches such as re-sampling, information augmentation, or model architecture modifications. \todo{not yet module improvement.}

% \section{Motivation}
% Discuss why the problem is significant, including real-world implications.
% Mention the importance of biodiversity studies or the challenges of species identification with limited samples.
% Discuss broader impacts, such as how solving long-tailed learning problems can benefit other fields.



\section{Related Work}
The challenge of long-tailed datasets has been extensively studied in the literature \cite{zhang2023deep,zhang2024systematicreviewlongtailedlearning}. % Zhang et al. \cite{zhang2023deep} conducted a comprehensive survey on long-tailed learning techniques, categorizing them into three main strategies: class re-balancing, information augmentation, and module improvement. This section explores class re-balancing approaches that are beyond the scope of this thesis, as well as other related methodologies commonly employed to address the issues of long-tailed datasets.

\paragraph{Data Re-sampling}
Data re-sampling techniques aim to modify the training dataset to mitigate class imbalance by simulating a balanced dataset. The most popular methods for re-sampling are random over-sampling (ROS) and random under-sampling (RUS) \cite{Chawla_2002,han2005}. ROS involves duplicating random samples from the minority classes, whereas RUS involves removing samples from the majority classes \cite{zhang2023deep,han2005}. However, these techniques have their weaknesses: over-sampling the minority classes will eventually lead to overfitting tail classes, and under-sampling can lead to underperformance on head classes \cite{zhang2023deep}. Another re-sampling technique, called Synthetic Minority Over-Sampling Technique (SMOTE) \cite{Chawla_2002}, involves synthetical generation of instances of the minority classes based on the existing data. 

\paragraph{Transfer Learning}
Transfer learning techniques adress the issue of class imblance by transferring learned features from head classes to tail classes. Examples include transferring the intra-class variance \cite{yin2019featuretransferlearningdeep} and transferring semantic deep features \cite{liu2019largescalelongtailedrecognitionopen}.

\paragraph{Data Augmentation}
Data augmentation is a deep learning technique used to expand the training dataset by applying transformations to samples or features, enhancing model accuracy while reducing overfitting \cite{perez2017effectivenessdataaugmentationimage,shorten2019survey}. Common augmentation methods include Mixup \cite{zhang2018mixupempiricalriskminimization}, which combines pairs of samples to create interpolated examples; CutMix \cite{yun2019cutmixregularizationstrategytrain}, which replaces regions of one image with patches from another; and UniMix \cite{xu2021calibratedmodellongtailedvisual}, which focuses on calibrating the feature space for long-tailed distributions. Rare-class Sample Generator (RSG) \cite{wang2021rsgsimpleeffectivemodule} focuses on enhancing tail-class performance by transferring knowledge from head classes.

\paragraph{Decoupled Training}
Decoupled training \cite{kang2020decouplingrepresentationclassifierlongtailed} separates representation learning and classifier training. First, the model is trained to extract features from the entire dataset, afterwards, a re-balancing technique is applied during classification. Despite the simple framework, decoupling showed state-of-the art performance on long-tailed benchmarks.

\paragraph{Ensemple Learning}
Ensemble learning is a technique that combines multiple network modules (i.e. experts) to solve long-tailed problems. By doing so, the predictions of the different models can be combined, ultimately outperforming a single model \cite{zhou2020bbnbilateralbranchnetworkcumulative,wang2022longtailedrecognitionroutingdiverse}.

% Few Shot Learning

\section{Reading Guide}
\todo{Mention what each chapter will cover and how they relate to each other.}
% Example: "Chapter 2 provides the theoretical background for long-tailed learning and deep learning methodologies. Chapter 3 describes the methodology used in this thesis, including dataset preparation and implementation details. Chapters 4 and 5 present the experimental results and analysis, followed by conclusions and suggestions for future work in Chapter 6."

