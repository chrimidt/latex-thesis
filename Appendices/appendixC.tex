\chapter{Experimental Setup Details}
% \section{Dataset Specifications}
\label{app:specs}
% Table \ref{tab:dataset_specs} provides an overview of the dataset splits used in this thesis. The training set consists of 45,000 samples, with 450 samples per class, generated by splitting the original CIFAR-100 training set into training and test sets. The test set derived from the split consists of 5,000 samples, with 50 samples per class. The validation set, unaltered from the original CIFAR-100 test set, contains 10,000 samples with an equal distribution of 100 samples per class.

% To simulate real-world class imbalance scenarios, an exponential imbalance was introduced into the training and test sets. The imbalance factor (\(\text{imb\_factor}\)) was set to 0.01, resulting in a significant reduction of samples for the least frequent classes. Table \ref{tab:imbalance_specs} provides a summary of the imbalance characteristics.

% Additionally, the imbalanced test set was further divided into three subsets based on class frequencies in the training data:

% \begin{itemize}
%     \item \textbf{Head Test Set:} Includes the top one-third most frequent classes.
%     \item \textbf{Middle Test Set:} Includes the middle one-third of classes.
%     \item \textbf{Tail Test Set:} Includes the bottom one-third least frequent classes.
% \end{itemize}

% \noindent The sample specifics are presented in table \ref{tab:dataset_specs}.

% % The class splits were determined by sorting classes based on the number of samples in the imbalanced training dataset and grouping them equally into three categories. The resulting test splits ensure that the model is evaluated on head, middle, and tail subsets, highlighting performance variations across different class frequencies.


% \begin{table}[h]
%     \centering
%     \caption{Dataset Specifications}
%     \begin{tabular}{|c|c|c|}
%     \hline
%     \textbf{Dataset Component} & \textbf{Total Samples} & \textbf{Samples per Class} \\ \hline
%     Training Set               & 45,000                & 450                        \\ \hline
%     Validation Set             & 10,000                & 100 
%     \\ \hline
%     Test Set                   & 5,000                 & 50                         \\ \hline
%     Head Test Set              & \todo{investigate}                  & Variable      
%     \\ \hline
%     Middle Test Set            & \todo{investigate}                  & Variable    
%     \\ \hline
%     Tail Test Set              & \todo{investigate}                  & Variable      
%     \\ \hline
%     \end{tabular}
%     \label{tab:dataset_specs}
%     \end{table}

% \begin{table}[h]
%         \centering
%         \caption{Imbalance Specifications}
%         \begin{tabular}{|c|l|}
%         \hline
%         \textbf{Aspect}             & \textbf{Details}                                         \\ \hline
%         Imbalance Type              & Exponential                                             \\ \hline
%         Imbalance Factor            & 0.01                                                   \\ \hline
%         Training Set Distribution   & 
%         \begin{tabular}[c]{@{}l@{}}
%             Most frequent class: 450 samples \\ 
%             Least frequent class: 4 samples
%         \end{tabular} \\ \hline
%         Test Set Distribution       & 
%         \begin{tabular}[c]{@{}l@{}}
%             Mirrors training distribution \\ 
%             Ensures no class has fewer than 1 sample
%         \end{tabular} \\ \hline
%         \end{tabular}
%         \label{tab:imbalance_specs}
% \end{table}


\section{Data Preprocessing}
Table \ref{tab:data_preprocessing} summarizes the preprocessing steps applied to the training, validation, and test datasets.

\begin{table}[h!]
    \centering
    \caption{Data Preprocessing Steps}
    \small
    \begin{tabular}{|c|l|}
    \hline
    \textbf{Dataset}      & \textbf{Preprocessing Steps}                                                                                          \\ \hline
    \textbf{Training}     & \begin{tabular}[c]{@{}l@{}}
    \textbullet\ Resize to 224×224 pixels \\ 
    \textbullet\ Random crop to 224×224 with 4 pixels of padding \\ 
    \textbullet\ Random horizontal flip \\ 
    \textbullet\ Normalize using CIFAR-100 statistics: \\ 
    \hspace{10pt} Mean = [0.4914, 0.4822, 0.4465], \\ 
    \hspace{10pt} Std = [0.2023, 0.1994, 0.2010]
    \end{tabular} \\ \hline
    \textbf{Validation/Test} & \begin{tabular}[c]{@{}l@{}}
    \textbullet\ Resize to 224×224 pixels \\ 
    \textbullet\ Normalize using CIFAR-100 statistics: \\ 
    \hspace{10pt} Mean = [0.4914, 0.4822, 0.4465], \\ 
    \hspace{10pt} Std = [0.2023, 0.1994, 0.2010]
    \end{tabular} \\ \hline
    \end{tabular}
    \label{tab:data_preprocessing}
    \end{table}


\section{Model Architecture Settings}
The specifications of the model architectures can be seen in table \ref{tab:model_settings}.

\begin{table}[h]
    \centering
    \caption{Model Architecture Settings}
    \small
    \begin{tabular}{|c|c|c|}
    \hline
    \textbf{Model Name}   & \textbf{Pretrained Weights}            & \textbf{Modifications for CIFAR-100}                     \\ \hline
    ResNet-50 \cite{pytorchresnet}            & \makecell[l]{\texttt{ResNet50\_Weights.} \\ \texttt{IMAGENET1K\_V2}} 
    & \makecell[l]{Replaced the \texttt{fc} layer \\ with a 100-class fully connected layer} \\ \hline
    MobileNetV2  \cite{pytorch_mobilenetv2}         & \makecell[l]{\texttt{MobileNet\_V2\_Weights.} \\ \texttt{IMAGENET1K\_V1}} 
                          & \makecell[l]{Replaced classification layer \\ with a 100-class fully connected layer} \\ \hline
    ViT-B/16 \cite{dosovitskiy2021imageworth16x16words}              & \makecell[l]{\texttt{timm vit\_base\_patch16\_224} \\ pretrained} 
                          & \makecell[l]{Replaced the \texttt{head} \\ with a 100-class fully connected layer} \\ \hline
    ConvNeXt-Base \cite{pytorch-convnext}       & \makecell[l]{\texttt{ConvNeXt\_Base\_Weights.} \\ \texttt{DEFAULT}} 
                          & \makecell[l]{Replaced the final layer of the \\ classifier with a 100-class fully \\ connected layer} \\ \hline
    \end{tabular}
    \label{tab:model_settings}
\end{table}



% \section{Evaluation Metrics}
% The primary metric used to evaluate model performance during validation and testing is the top-1 accuracy. Alongside, the F1 score was calculated (macro F1 for balanced datasets, and weighted F1 for imbalanced dataset) but never used for evaluation. The F1 scores for all experiments can be seen in appendix \ref{app:A}.
% The following metrics were used to evaluate model performance during validation and testing:

% \begin{itemize}
%     \item \textbf{Top-1 Accuracy}: Proportion of correct predictions across all classes.
%     \item \textbf{F1 Score}:
%     \begin{itemize}
%         \item \textbf{Macro F1}: Used for balanced datasets, e.g., validation set and balanced test set.
%         \item \textbf{Weighted F1}: Used for imbalanced datasets to account for class frequency differences.
%     \end{itemize}
%     \item \textbf{Head/Middle/Tail Class Evaluation}: 
%     \begin{itemize}
%         \item The test set was divided into three subsets: head, middle, and tail classes, based on class frequency in the training dataset.
%         \item Performance on each subset was evaluated using both:
%         \begin{itemize}
%             \item \textbf{Top-1 Accuracy}
%             \item \textbf{Weighted F1 Score}
%         \end{itemize}
%     \end{itemize}
% \end{itemize}


\section{Hardware and Software Configurations}
\label{sec:hardware_software}
Tables \ref{tab:hardware-specs} and \ref{tab:software-specs} lists the hardware and software specifactions for the experiments in this thesis, respectively.

\begin{table}[h!]
    \centering
    \caption{Hardware Specifications}
    \small
    \begin{tabular}{|l|l|}
    \hline
    \textbf{Component}        & \textbf{Specification}                  \\ \hline
    GPUs                      & 4 NVIDIA TITAN X (Pascal), 12 GB each   \\ \hline
    RAM                       & 125 GiB                                \\ \hline
    Swap Space                & 63 GiB                                 \\ \hline
    CUDA Version              & 12.4                                   \\ \hline
    Driver Version            & 550.90.07                              \\ \hline
    \end{tabular}
    \label{tab:hardware-specs}
    \end{table}
    



\begin{table}[h!]
    \centering
    \caption{Software Specifications}
    \small
    \begin{tabular}{|l|l|}
    \hline
    \textbf{Component}                      & \textbf{Specification}                          \\ \hline
    Operating System                        & Ubuntu 22.04.4 LTS (Jammy Jellyfish)            \\ \hline
    Python Version                          & 3.11.8                                         \\ \hline
    Libraries  & \begin{tabular}[c]{@{}l@{}}
    PyTorch (\(\geq\) 1.7) \\ 
    Torchvision (\(\geq\) 0.8.0) \\ 
    Timm (1.0.11) \\ 
    Tensorboard (\(\geq\) 1.14)
    \end{tabular} \\ \hline
    \end{tabular}
    \label{tab:software-specs}
    \end{table}
    


\section{Reproducibility Considerations}
To ensure that the experiments conducted in this thesis are reproducible, the following measures were implemented:

\begin{itemize}
    \item \textbf{Random Seed}: 
    \begin{itemize}
        \item A fixed random seed of \texttt{42} was used for all experiments to ensure consistent initialization across runs.
        \item Randomness was controlled for:
        \begin{itemize}
            \item Python's \texttt{random} library.
            \item NumPy (\texttt{np.random.seed}).
            \item PyTorch (\texttt{torch.manual\_seed} and \texttt{torch.cuda.manual\_seed}).
        \end{itemize}
        \item \texttt{cudnn.deterministic} was set to \texttt{True} to enforce deterministic behavior in GPU computations.
    \end{itemize}
    \item \textbf{Configuration Management}:
    \begin{itemize}
        \item All hyperparameters, dataset settings, and model configurations were defined in YAML configuration files.
    \end{itemize}
    \item \textbf{Saved Artifacts}:
    \begin{itemize}
        \item Datasets were saved, making them accessible for evaluation or reuse in future experiments.
        \item Model checkpoints were saved after achieving the best validation accuracy.
    \end{itemize}
\end{itemize}
